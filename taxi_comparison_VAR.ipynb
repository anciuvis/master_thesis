{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Demand Prediction - All Models vs All Clusters\n",
    "## Master Thesis - Vilnius University\n",
    "\n",
    "This notebook trains 3 models on ALL 100 NYC taxi clusters for comparison.\n",
    "\n",
    "**Models:**\n",
    "- ConvLSTM (spatiotemporal learning)\n",
    "- XGBoost (gradient boosting with features)\n",
    "- Vector AutoRegression VAR (multivariate temporal model capturing spillovers)\n",
    "\n",
    "**Output:** Comparative analysis showing which model works best for each cluster type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical models\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paths\n",
    "INPUT_PATH = 'C:/Users/Anya/master_thesis/output'\n",
    "OUTPUT_PATH = 'C:/Users/Anya/master_thesis/output/models_all_comparison'\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_PATH, 'checkpoints')\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "N_LAGS = 24\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENVIRONMENT CONFIGURED - ALL MODELS COMPARISON (XGBoost + VAR + ConvLSTM)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✓ Output directory: {OUTPUT_PATH}\")\n",
    "print(f\"✓ Will train 3 models: XGBoost (100 clusters), VAR (top 20), ConvLSTM (top 20)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 1: Load & Prepare Data for All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Loading Data for All 100 Clusters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load raw data\n",
    "data = pd.read_parquet(os.path.join(INPUT_PATH, 'taxi_data_with_clusters_full.parquet'))\n",
    "print(f\"\\nRaw data loaded:\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Date range: {data['tpep_pickup_datetime'].min()} to {data['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"  Clusters: {data['kmeans_cluster'].nunique()}\")\n",
    "\n",
    "# Aggregate to hourly demand by cluster\n",
    "data['time_period'] = data['tpep_pickup_datetime'].dt.floor('H')\n",
    "demand = data.groupby(['time_period', 'kmeans_cluster']).size().reset_index(name='demand')\n",
    "demand_matrix = demand.pivot(index='time_period', columns='kmeans_cluster', values='demand').fillna(0)\n",
    "demand_matrix = demand_matrix.sort_index()\n",
    "\n",
    "print(f\"\\nDemand matrix created:\")\n",
    "print(f\"  Shape: {demand_matrix.shape}\")\n",
    "print(f\"  All clusters: {demand_matrix.shape[1]}\")\n",
    "print(f\"  Memory size: {demand_matrix.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Temporal split (70% train, 10% val, 20% test)\n",
    "n = len(demand_matrix)\n",
    "train_end = int(n * (1 - TEST_SIZE - VAL_SIZE))\n",
    "val_end = int(n * (1 - TEST_SIZE))\n",
    "\n",
    "train_data_all = demand_matrix.iloc[:train_end]\n",
    "val_data_all = demand_matrix.iloc[train_end:val_end]\n",
    "test_data_all = demand_matrix.iloc[val_end:]\n",
    "\n",
    "print(f\"\\nTrain-Test Split:\")\n",
    "print(f\"  Training: {len(train_data_all)} hours\")\n",
    "print(f\"  Validation: {len(val_data_all)} hours\")\n",
    "print(f\"  Test: {len(test_data_all)} hours\")\n",
    "\n",
    "# Save checkpoint\n",
    "demand_matrix.to_pickle(os.path.join(CHECKPOINT_PATH, '01_demand_matrix.pkl'))\n",
    "train_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_train_data.pkl'))\n",
    "val_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_val_data.pkl'))\n",
    "test_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_test_data.pkl'))\n",
    "\n",
    "print(f\"\\n✓ Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 2: Analyze Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Analyze Cluster Characteristics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate statistics for each cluster\n",
    "cluster_stats = pd.DataFrame({\n",
    "    'cluster_id': demand_matrix.columns,\n",
    "    'avg_hourly_demand': demand_matrix.mean(),\n",
    "    'median_hourly_demand': demand_matrix.median(),\n",
    "    'max_hourly_demand': demand_matrix.max(),\n",
    "    'std_hourly_demand': demand_matrix.std(),\n",
    "    'total_demand': demand_matrix.sum(),\n",
    "    'sparsity_pct': (demand_matrix == 0).sum() / len(demand_matrix) * 100,\n",
    "    'non_zero_hours': (demand_matrix != 0).sum()\n",
    "}).sort_values('total_demand', ascending=False).reset_index(drop=True)\n",
    "\n",
    "cluster_stats['demand_category'] = pd.cut(\n",
    "    cluster_stats['avg_hourly_demand'],\n",
    "    bins=[0, 10, 50, 500],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(f\"\\nCluster Demand Categories:\")\n",
    "print(f\"  High-demand (>50 trips/hour): {(cluster_stats['demand_category'] == 'High').sum()} clusters\")\n",
    "print(f\"  Medium-demand (10-50 trips/hour): {(cluster_stats['demand_category'] == 'Medium').sum()} clusters\")\n",
    "print(f\"  Low-demand (<10 trips/hour): {(cluster_stats['demand_category'] == 'Low').sum()} clusters\")\n",
    "\n",
    "# Save statistics\n",
    "cluster_stats.to_csv(os.path.join(OUTPUT_PATH, 'cluster_characteristics.csv'), index=False)\n",
    "print(f\"\\n✓ Cluster characteristics saved\")\n",
    "\n",
    "print(f\"\\nTop 10 clusters:\")\n",
    "print(cluster_stats.head(10)[['cluster_id', 'avg_hourly_demand', 'sparsity_pct', 'demand_category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 3: Train XGBoost on All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: XGBoost Training (All 100 Clusters)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_xgboost_features(data_df, n_lags=24):\n",
    "    \"\"\"Create lag features for XGBoost\"\"\"\n",
    "    df = data_df.copy()\n",
    "    \n",
    "    # Create lags\n",
    "    for col in data_df.columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df[f'{col}_lag_{lag}'] = data_df[col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for col in data_df.columns:\n",
    "        df[f'{col}_rolling_mean_6'] = data_df[col].shift(1).rolling(window=6).mean()\n",
    "        df[f'{col}_rolling_std_6'] = data_df[col].shift(1).rolling(window=6).std()\n",
    "        df[f'{col}_rolling_mean_24'] = data_df[col].shift(1).rolling(window=24).mean()\n",
    "    \n",
    "    # Temporal features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "print(\"\\nCreating XGBoost features...\")\n",
    "train_features = create_xgboost_features(train_data_all, N_LAGS)\n",
    "val_features = create_xgboost_features(val_data_all, N_LAGS)\n",
    "test_features = create_xgboost_features(test_data_all, N_LAGS)\n",
    "\n",
    "feature_cols = [col for col in train_features.columns if col not in demand_matrix.columns]\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Train XGBoost for each cluster\n",
    "xgb_models = {}\n",
    "xgb_metrics = {}\n",
    "xgb_predictions = pd.DataFrame(index=test_features.index)\n",
    "\n",
    "print(f\"\\nTraining {len(demand_matrix.columns)} XGBoost models...\")\n",
    "for i, cluster in enumerate(demand_matrix.columns):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Progress: {i+1}/100\")\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        train_features[feature_cols], \n",
    "        train_features[cluster],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    xgb_models[cluster] = model\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(test_features[feature_cols])\n",
    "    xgb_predictions[cluster] = pred\n",
    "    \n",
    "    # Evaluate\n",
    "    actual = test_features[cluster].values\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - pred[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    xgb_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Summary\n",
    "xgb_mape_values = [m['MAPE'] for m in xgb_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(f\"\\nXGBoost Summary (All 100 Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Std MAPE: {np.std(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Min MAPE: {np.min(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Max MAPE: {np.max(xgb_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(OUTPUT_PATH, 'xgboost_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_models, f)\n",
    "with open(os.path.join(OUTPUT_PATH, 'xgboost_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_metrics, f)\n",
    "\n",
    "del train_features, val_features, test_features\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ XGBoost models trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 4: Train Vector AutoRegression (VAR) on Top 20 High-Demand Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: Vector AutoRegression (VAR) - Top 20 High-Demand Zones\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top 20 high-demand clusters\n",
    "top_20_clusters = cluster_stats.head(20)['cluster_id'].tolist()\n",
    "\n",
    "print(f\"\\nVAR on {len(top_20_clusters)} high-demand clusters\")\n",
    "print(f\"  Reason: VAR captures spatial spillovers (taxi movements between zones)\")\n",
    "print(f\"  Example: Taxi shortage at LaGuardia → higher demand in Midtown\")\n",
    "\n",
    "# Prepare data subsets\n",
    "train_var = train_data_all[top_20_clusters]\n",
    "test_var = test_data_all[top_20_clusters]\n",
    "\n",
    "print(f\"\\nVAR Data:\")\n",
    "print(f\"  Training shape: {train_var.shape}\")\n",
    "print(f\"  Test shape: {test_var.shape}\")\n",
    "print(f\"  Clusters: {len(top_20_clusters)}\")\n",
    "\n",
    "# Check stationarity (VAR requirement)\n",
    "print(f\"\\nChecking stationarity (Augmented Dickey-Fuller test)...\")\n",
    "stationary_count = 0\n",
    "for col in train_var.columns:\n",
    "    _, p_value, _, _, _ = adfuller(train_var[col].dropna(), autolag='AIC')\n",
    "    if p_value < 0.05:\n",
    "        stationary_count += 1\n",
    "\n",
    "print(f\"  Stationary series: {stationary_count}/{len(train_var.columns)}\")\n",
    "\n",
    "# Difference if needed for stationarity\n",
    "def ensure_stationary_for_var(data, max_diff=2):\n",
    "    \"\"\"Difference data until stationary for VAR\"\"\"\n",
    "    for d in range(max_diff + 1):\n",
    "        if d == 0:\n",
    "            diff_data = data.copy()\n",
    "        else:\n",
    "            diff_data = data.diff(d).dropna()\n",
    "        \n",
    "        # Check if most series stationary\n",
    "        adf_results = []\n",
    "        for col in diff_data.columns:\n",
    "            _, p_value, _, _, _ = adfuller(diff_data[col].dropna(), autolag='AIC')\n",
    "            adf_results.append(p_value < 0.05)\n",
    "        \n",
    "        if sum(adf_results) >= len(diff_data.columns) * 0.8:\n",
    "            return diff_data, d\n",
    "    return data, 0\n",
    "\n",
    "train_var_stat, d_order = ensure_stationary_for_var(train_var)\n",
    "print(f\"  Differencing order applied: d={d_order}\")\n",
    "\n",
    "# Fit VAR model\n",
    "print(f\"\\nFitting VAR model with optimal lag selection...\")\n",
    "var_model = VAR(train_var_stat)\n",
    "\n",
    "# Select optimal lag order\n",
    "lag_order_results = var_model.select_order(maxlags=24)\n",
    "optimal_lag_aic = lag_order_results.aic\n",
    "optimal_lag_bic = lag_order_results.bic\n",
    "\n",
    "print(f\"  Optimal lag (AIC): {optimal_lag_aic}\")\n",
    "print(f\"  Optimal lag (BIC): {optimal_lag_bic}\")\n",
    "\n",
    "# Fit with AIC lag (captures more dynamics)\n",
    "var_results = var_model.fit(optimal_lag_aic)\n",
    "\n",
    "print(f\"\\nVAR Model Summary:\")\n",
    "print(f\"  Number of equations: {var_results.neqs}\")\n",
    "print(f\"  Number of observations: {var_results.nobs}\")\n",
    "print(f\"  AIC: {var_results.aic:.4f}\")\n",
    "print(f\"  BIC: {var_results.bic:.4f}\")\n",
    "\n",
    "# Check stability\n",
    "eigenvalues = var_results.roots\n",
    "is_stable = np.all(np.abs(eigenvalues) < 1)\n",
    "print(f\"  Model stable: {'Yes ✓' if is_stable else 'No ✗ (check results)'}\")\n",
    "print(f\"  Max eigenvalue: {np.max(np.abs(eigenvalues)):.4f}\")\n",
    "\n",
    "# Forecast on test period\n",
    "print(f\"\\nGenerating VAR forecasts...\")\n",
    "var_pred_list = []\n",
    "\n",
    "# Use last observations from training data to start forecasting\n",
    "last_obs = train_var_stat.iloc[-optimal_lag_aic:].values\n",
    "\n",
    "for i in range(len(test_var)):\n",
    "    if i % 50 == 0 and i > 0:\n",
    "        print(f\"  Progress: {i}/{len(test_var)}\")\n",
    "    \n",
    "    # Forecast 1 step ahead\n",
    "    forecast = var_results.forecast(last_obs, steps=1)\n",
    "    var_pred_list.append(forecast[0])\n",
    "    \n",
    "    # Update for next forecast (rolling window)\n",
    "    last_obs = np.vstack([last_obs[1:], forecast])\n",
    "\n",
    "var_predictions_arr = np.array(var_pred_list)\n",
    "\n",
    "# Inverse difference if we differenced the data\n",
    "if d_order > 0:\n",
    "    # Add back the trend from training data\n",
    "    var_predictions_undiff = var_predictions_arr.copy()\n",
    "    for d in range(d_order):\n",
    "        var_predictions_undiff = np.cumsum(var_predictions_undiff, axis=0) + train_var.iloc[-1].values\n",
    "    var_predictions_arr = var_predictions_undiff\n",
    "\n",
    "# Create predictions dataframe\n",
    "var_predictions = pd.DataFrame(\n",
    "    var_predictions_arr,\n",
    "    index=test_var.index,\n",
    "    columns=top_20_clusters\n",
    ")\n",
    "\n",
    "# Evaluate VAR\n",
    "var_metrics = {}\n",
    "var_mape_values = []\n",
    "\n",
    "print(f\"\\nEvaluating VAR predictions...\")\n",
    "for cluster in top_20_clusters:\n",
    "    actual = test_var[cluster].values\n",
    "    pred = var_predictions[cluster].values\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - pred[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    var_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "    if not np.isnan(mape):\n",
    "        var_mape_values.append(mape)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nVAR Summary (Top 20 High-Demand Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(var_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(var_mape_values):.2f}%\")\n",
    "print(f\"  Std MAPE: {np.std(var_mape_values):.2f}%\")\n",
    "print(f\"  Min MAPE: {np.min(var_mape_values):.2f}%\")\n",
    "print(f\"  Max MAPE: {np.max(var_mape_values):.2f}%\")\n",
    "\n",
    "# Granger Causality Analysis\n",
    "print(f\"\\nGranger Causality Analysis...\")\n",
    "print(f\"  Testing if Zone j Granger-causes Zone i (does j help predict i?)\\n\")\n",
    "\n",
    "causality_pairs = []\n",
    "for i, effect_cluster in enumerate(top_20_clusters[:5]):  # Show first 5 for brevity\n",
    "    for j, cause_cluster in enumerate(top_20_clusters[:5]):\n",
    "        if i != j:\n",
    "            try:\n",
    "                gc_result = grangercausalitytests(\n",
    "                    train_var[[effect_cluster, cause_cluster]].dropna(),\n",
    "                    maxlag=7,\n",
    "                    verbose=False\n",
    "                )\n",
    "                min_p_value = min([gc_result[lag][0][0][1] for lag in range(1, 8)])\n",
    "                if min_p_value < 0.05:\n",
    "                    causality_pairs.append({\n",
    "                        'cause': int(cause_cluster),\n",
    "                        'effect': int(effect_cluster),\n",
    "                        'p_value': min_p_value\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "if causality_pairs:\n",
    "    causality_df = pd.DataFrame(causality_pairs).sort_values('p_value')\n",
    "    print(\"  Significant causal relationships (p < 0.05):\")\n",
    "    for idx, row in causality_df.head(10).iterrows():\n",
    "        print(f\"    Zone {row['cause']:.0f} → Zone {row['effect']:.0f} (p={row['p_value']:.4f})\")\nelse:\n",
    "    print(\"  No significant causal relationships detected in top 5 clusters\")\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(OUTPUT_PATH, 'var_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(var_metrics, f)\n",
    "\n",
    "var_predictions.to_csv(os.path.join(OUTPUT_PATH, 'var_predictions.csv'))\n",
    "\n",
    "print(f\"\\n✓ VAR model trained and saved\")\n",
    "print(f\"  Key insight: VAR captures spatial spillovers via Granger causality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 5: Train ConvLSTM on Top 20 High-Demand Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: ConvLSTM Training (Top 20 High-Demand Clusters)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nConvLSTM on {len(top_20_clusters)} high-demand clusters\")\n",
    "print(f\"  Reason: ConvLSTM computationally expensive, best on rich data\")\n",
    "\n",
    "# Prepare data\n",
    "train_convlstm = train_data_all[top_20_clusters]\n",
    "val_convlstm = val_data_all[top_20_clusters]\n",
    "test_convlstm = test_data_all[top_20_clusters]\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(train_convlstm),\n",
    "    index=train_convlstm.index,\n",
    "    columns=train_convlstm.columns\n",
    ")\n",
    "val_scaled = pd.DataFrame(\n",
    "    scaler.transform(val_convlstm),\n",
    "    index=val_convlstm.index,\n",
    "    columns=val_convlstm.columns\n",
    ")\n",
    "test_scaled = pd.DataFrame(\n",
    "    scaler.transform(test_convlstm),\n",
    "    index=test_convlstm.index,\n",
    "    columns=test_convlstm.columns\n",
    ")\n",
    "\n",
    "# Create grid (5x4 = 20 clusters)\n",
    "def create_grid_data(data_df, grid_height=5, grid_width=4):\n",
    "    n_timesteps = len(data_df)\n",
    "    grid_data = np.zeros((n_timesteps, grid_height, grid_width, 1))\n",
    "    for idx, col in enumerate(data_df.columns):\n",
    "        row = idx // grid_width\n",
    "        col_pos = idx % grid_width\n",
    "        if row < grid_height:\n",
    "            grid_data[:, row, col_pos, 0] = data_df[col].values\n",
    "    return grid_data\n",
    "\n",
    "grid_train = create_grid_data(train_scaled, 5, 4)\n",
    "grid_val = create_grid_data(val_scaled, 5, 4)\n",
    "grid_test = create_grid_data(test_scaled, 5, 4)\n",
    "\n",
    "print(f\"\\nGrid shapes: {grid_train.shape}\")\n",
    "\n",
    "# Build ConvLSTM model\n",
    "input_layer = Input(shape=(6, 5, 4, 1), name='input')\n",
    "x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True, activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "output = Conv2D(1, (1, 1), padding='same', activation='relu', name='output')(x)\n",
    "\n",
    "convlstm_model = Model(inputs=input_layer, outputs=output)\n",
    "convlstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"\\nTraining ConvLSTM...\")\n",
    "history = convlstm_model.fit(\n",
    "    grid_train, grid_train,\n",
    "    validation_data=(grid_val, grid_val),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict\n",
    "convlstm_pred = convlstm_model.predict(grid_test, verbose=0)\n",
    "\n",
    "# Extract and evaluate\n",
    "convlstm_metrics = {}\n",
    "convlstm_predictions = pd.DataFrame(index=test_convlstm.index)\n",
    "\n",
    "for idx, cluster in enumerate(top_20_clusters):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    \n",
    "    pred_grid = convlstm_pred[:, row, col, 0]\n",
    "    convlstm_predictions[cluster] = scaler.inverse_transform(\n",
    "        np.column_stack([pred_grid] * len(train_convlstm.columns))\n",
    "    )[:, 0]\n",
    "    \n",
    "    actual = test_convlstm[cluster].values\n",
    "    rmse = np.sqrt(mean_squared_error(actual, convlstm_predictions[cluster].values))\n",
    "    mae = mean_absolute_error(actual, convlstm_predictions[cluster].values)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - convlstm_predictions[cluster].values[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    convlstm_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Summary\n",
    "convlstm_mape_values = [m['MAPE'] for m in convlstm_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(f\"\\nConvLSTM Summary (Top 20 High-Demand Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(convlstm_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(convlstm_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "convlstm_model.save(os.path.join(OUTPUT_PATH, 'convlstm_model.keras'))\n",
    "with open(os.path.join(OUTPUT_PATH, 'convlstm_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(convlstm_metrics, f)\n",
    "\n",
    "print(f\"\\n✓ ConvLSTM model trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 6: Comparative Analysis & Model Performance Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS: Which Model Works Best for Each Cluster?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for cluster in demand_matrix.columns:\n",
    "    row = {'cluster_id': cluster}\n",
    "    \n",
    "    # Get demand category\n",
    "    cluster_info = cluster_stats[cluster_stats['cluster_id'] == cluster].iloc[0]\n",
    "    row['avg_demand'] = cluster_info['avg_hourly_demand']\n",
    "    row['demand_category'] = cluster_info['demand_category']\n",
    "    row['sparsity_pct'] = cluster_info['sparsity_pct']\n",
    "    \n",
    "    # XGBoost metrics\n",
    "    if cluster in xgb_metrics:\n",
    "        row['xgboost_mape'] = xgb_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['xgboost_mape'] = np.nan\n",
    "    \n",
    "    # VAR metrics (only for top 20 clusters)\n",
    "    if cluster in var_metrics:\n",
    "        row['var_mape'] = var_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['var_mape'] = np.nan\n",
    "    \n",
    "    # ConvLSTM metrics (only for top 20 clusters)\n",
    "    if cluster in convlstm_metrics:\n",
    "        row['convlstm_mape'] = convlstm_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['convlstm_mape'] = np.nan\n",
    "    \n",
    "    # Determine best model\n",
    "    mapes = {\n",
    "        'XGBoost': row['xgboost_mape'],\n",
    "        'VAR': row['var_mape'],\n",
    "        'ConvLSTM': row['convlstm_mape']\n",
    "    }\n",
    "    # Remove NaN values\n",
    "    mapes = {k: v for k, v in mapes.items() if not np.isnan(v)}\n",
    "    \n",
    "    if mapes:\n",
    "        row['best_model'] = min(mapes, key=mapes.get)\n",
    "        row['best_mape'] = min(mapes.values())\n",
    "    else:\n",
    "        row['best_model'] = 'N/A'\n",
    "        row['best_mape'] = np.nan\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(os.path.join(OUTPUT_PATH, 'model_comparison_all_clusters.csv'), index=False)\n",
    "\n",
    "print(f\"\\nComparison Results:\")\n",
    "print(f\"\\nBest Model Distribution:\")\n",
    "print(comparison_df['best_model'].value_counts())\n",
    "\n",
    "print(f\"\\nPerformance by Model (MAPE):\")\n",
    "print(f\"  XGBoost - Mean: {comparison_df['xgboost_mape'].mean():.2f}%, Median: {comparison_df['xgboost_mape'].median():.2f}%\")\n",
    "print(f\"  VAR     - Mean: {comparison_df['var_mape'].mean():.2f}%, Median: {comparison_df['var_mape'].median():.2f}%\")\n",
    "print(f\"  ConvLSTM - Mean: {comparison_df['convlstm_mape'].mean():.2f}%, Median: {comparison_df['convlstm_mape'].median():.2f}%\")\n",
    "\n",
    "print(f\"\\nBest Model by Demand Category:\")\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    subset = comparison_df[comparison_df['demand_category'] == category]\n",
    "    if len(subset) > 0:\n",
    "        best_counts = subset['best_model'].value_counts()\n",
    "        print(f\"  {category}: {dict(best_counts)}\")\n",
    "\n",
    "print(f\"\\n✓ Comparison saved to model_comparison_all_clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 7: Summary Tables & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY & INSIGHTS FOR THESIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "summary_stats = {\n",
    "    'Model': ['XGBoost', 'VAR (High-Demand)', 'ConvLSTM (High-Demand)'],\n",
    "    'Clusters_Trained': [100, 20, 20],\n",
    "    'Mean_MAPE': [\n",
    "        comparison_df['xgboost_mape'].mean(),\n",
    "        comparison_df['var_mape'].mean(),\n",
    "        comparison_df['convlstm_mape'].mean()\n",
    "    ],\n",
    "    'Median_MAPE': [\n",
    "        comparison_df['xgboost_mape'].median(),\n",
    "        comparison_df['var_mape'].median(),\n",
    "        comparison_df['convlstm_mape'].median()\n",
    "    ],\n",
    "    'Std_MAPE': [\n",
    "        comparison_df['xgboost_mape'].std(),\n",
    "        comparison_df['var_mape'].std(),\n",
    "        comparison_df['convlstm_mape'].std()\n",
    "    ],\n",
    "    'Best_for_Count': [\n",
    "        (comparison_df['best_model'] == 'XGBoost').sum(),\n",
    "        (comparison_df['best_model'] == 'VAR').sum(),\n",
    "        (comparison_df['best_model'] == 'ConvLSTM').sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_PATH, 'model_performance_summary.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Insights\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FOR THESIS CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_best = (comparison_df['best_model'] == 'XGBoost').sum()\n",
    "var_best = (comparison_df['best_model'] == 'VAR').sum()\n",
    "lstm_best = (comparison_df['best_model'] == 'ConvLSTM').sum()\n",
    "\n",
    "print(f\"\\n1. OVERALL PERFORMANCE:\")\n",
    "print(f\"   XGBoost wins for {xgb_best} clusters\")\n",
    "print(f\"   VAR wins for {var_best} clusters (captures spillovers)\")\n",
    "print(f\"   ConvLSTM wins for {lstm_best} clusters (spatiotemporal)\")\n",
    "\n",
    "print(f\"\\n2. MODEL SPECIALIZATION:\")\n",
    "high_dem = comparison_df[comparison_df['demand_category'] == 'High']\n",
    "med_dem = comparison_df[comparison_df['demand_category'] == 'Medium']\n",
    "low_dem = comparison_df[comparison_df['demand_category'] == 'Low']\n",
    "\n",
    "if len(high_dem) > 0:\n",
    "    print(f\"   High-demand zones: {high_dem['best_model'].value_counts().to_dict()}\")\n",
    "if len(med_dem) > 0:\n",
    "    print(f\"   Medium-demand zones: {med_dem['best_model'].value_counts().to_dict()}\")\n",
    "if len(low_dem) > 0:\n",
    "    print(f\"   Low-demand zones: {low_dem['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n3. SPARSE VS DENSE DATA:\")\n",
    "sparse = comparison_df[comparison_df['sparsity_pct'] > 30]\n",
    "dense = comparison_df[comparison_df['sparsity_pct'] <= 30]\n",
    "if len(sparse) > 0:\n",
    "    print(f\"   Sparse zones (>30% zeros): {sparse['best_model'].value_counts().to_dict()}\")\n",
    "if len(dense) > 0:\n",
    "    print(f\"   Dense zones (≤30% zeros): {dense['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n4. PERFORMANCE RANGE:\")\n",
    "print(f\"   XGBoost MAPE: {comparison_df['xgboost_mape'].min():.2f}% - {comparison_df['xgboost_mape'].max():.2f}%\")\n",
    "print(f\"   VAR MAPE: {comparison_df['var_mape'].min():.2f}% - {comparison_df['var_mape'].max():.2f}%\")\n",
    "print(f\"   ConvLSTM MAPE: {comparison_df['convlstm_mape'].min():.2f}% - {comparison_df['convlstm_mape'].max():.2f}%\")\n",
    "\n",
    "print(f\"\\n5. VAR ADVANTAGES:\")\n",
    "print(f\"   ✓ Captures spatial spillovers (taxi movements between zones)\")\n",
    "print(f\"   ✓ Granger causality reveals predictive relationships\")\n",
    "print(f\"   ✓ Economic interpretation: which zones drive others\")\n",
    "print(f\"   ⚠ Computational cost: limited to 20 zones (curse of dimensionality)\")\n",
    "\n",
    "print(f\"\\n✓ Summary saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 8: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['XGBoost', 'VAR', 'ConvLSTM']\n",
    "means = [\n",
    "    comparison_df['xgboost_mape'].mean(),\n",
    "    comparison_df['var_mape'].mean(),\n",
    "    comparison_df['convlstm_mape'].mean()\n",
    "]\n",
    "stds = [\n",
    "    comparison_df['xgboost_mape'].std(),\n",
    "    comparison_df['var_mape'].std(),\n",
    "    comparison_df['convlstm_mape'].std()\n",
    "]\n",
    "ax.bar(models, means, yerr=stds, capsize=10, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Mean MAPE (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison (Mean ± Std)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Best Model Distribution\n",
    "ax = axes[0, 1]\n",
    "best_counts = comparison_df['best_model'].value_counts()\n",
    "colors_dict = {'XGBoost': '#3498db', 'VAR': '#e74c3c', 'ConvLSTM': '#2ecc71'}\n",
    "colors = [colors_dict.get(idx, '#95a5a6') for idx in best_counts.index]\n",
    "ax.bar(best_counts.index, best_counts.values, color=colors)\n",
    "ax.set_ylabel('Number of Clusters', fontsize=12)\n",
    "ax.set_title('Best Model Distribution (Which Model Wins)', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(best_counts.values):\n",
    "    ax.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Performance by Demand Category\n",
    "ax = axes[1, 0]\n",
    "categories = ['High', 'Medium', 'Low']\n",
    "xgb_means = []\n",
    "var_means = []\n",
    "lstm_means = []\n",
    "for cat in categories:\n",
    "    subset = comparison_df[comparison_df['demand_category'] == cat]\n",
    "    if len(subset) > 0:\n",
    "        xgb_means.append(subset['xgboost_mape'].mean())\n",
    "        var_means.append(subset['var_mape'].mean())\n",
    "        lstm_means.append(subset['convlstm_mape'].mean())\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "ax.bar(x - width, xgb_means, width, label='XGBoost', color='#3498db')\n",
    "ax.bar(x, var_means, width, label='VAR', color='#e74c3c')\n",
    "ax.bar(x + width, lstm_means, width, label='ConvLSTM', color='#2ecc71')\n",
    "ax.set_ylabel('Mean MAPE (%)', fontsize=12)\n",
    "ax.set_title('Performance by Demand Category', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. MAPE Distribution Box Plot\n",
    "ax = axes[1, 1]\n",
    "data_to_plot = [\n",
    "    comparison_df['xgboost_mape'].dropna(),\n",
    "    comparison_df['var_mape'].dropna(),\n",
    "    comparison_df['convlstm_mape'].dropna()\n",
    "]\n",
    "bp = ax.boxplot(data_to_plot, labels=['XGBoost', 'VAR', 'ConvLSTM'], patch_artist=True)\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax.set_title('MAPE Distribution by Model', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'model_comparison_visualization.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Visualizations saved\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"ALL ANALYSIS COMPLETE - XGBoost + VAR + ConvLSTM Comparison\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"\\nOutput files saved to: {OUTPUT_PATH}\")\n",
    "print(f\"\\nKey files:\")\n",
    "print(f\"  - model_comparison_all_clusters.csv (cluster-by-cluster comparison)\")\n",
    "print(f\"  - model_performance_summary.csv (overall statistics)\")\n",
    "print(f\"  - var_predictions.csv (VAR forecasts for top 20 zones)\")\n",
    "print(f\"  - cluster_characteristics.csv (demand analysis)\")\n",
    "print(f\"  - model_comparison_visualization.png (charts)\")\n",
    "print(f\"\\nThesis Insight:\")\n",
    "print(f\"  VAR captures spatial spillovers (Granger causality) not seen by univariate models\")\n",
    "print(f\"  Better for policy analysis: which zones drive demand elsewhere\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
