{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Demand Prediction - All Models vs All Clusters\n",
    "## Master Thesis - Vilnius University\n",
    "\n",
    "This notebook trains 3 models on ALL 100 NYC taxi clusters for comparison.\n",
    "\n",
    "**Models:**\n",
    "- ConvLSTM (spatiotemporal learning)\n",
    "- XGBoost (gradient boosting with features)\n",
    "- AutoRegressive AR(7) (temporal baseline)\n",
    "\n",
    "**Output:** Comparative analysis showing which model works best for each cluster type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical models\n",
    "from statsmodels.tsa.api import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paths\n",
    "INPUT_PATH = 'C:/Users/Anya/master_thesis/output'\n",
    "OUTPUT_PATH = 'C:/Users/Anya/master_thesis/output/models_all_comparison'\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_PATH, 'checkpoints')\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "N_LAGS = 24\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENVIRONMENT CONFIGURED - ALL MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✓ Output directory: {OUTPUT_PATH}\")\n",
    "print(f\"✓ Will train 3 models on ALL clusters for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 1: Load & Prepare Data for All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Loading Data for All 100 Clusters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load raw data\n",
    "data = pd.read_parquet(os.path.join(INPUT_PATH, 'taxi_data_with_clusters_full.parquet'))\n",
    "print(f\"\\nRaw data loaded:\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Date range: {data['tpep_pickup_datetime'].min()} to {data['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"  Clusters: {data['kmeans_cluster'].nunique()}\")\n",
    "\n",
    "# Aggregate to hourly demand by cluster\n",
    "data['time_period'] = data['tpep_pickup_datetime'].dt.floor('H')\n",
    "demand = data.groupby(['time_period', 'kmeans_cluster']).size().reset_index(name='demand')\n",
    "demand_matrix = demand.pivot(index='time_period', columns='kmeans_cluster', values='demand').fillna(0)\n",
    "demand_matrix = demand_matrix.sort_index()\n",
    "\n",
    "print(f\"\\nDemand matrix created:\")\n",
    "print(f\"  Shape: {demand_matrix.shape}\")\n",
    "print(f\"  All clusters: {demand_matrix.shape[1]}\")\n",
    "print(f\"  Memory size: {demand_matrix.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Temporal split (70% train, 10% val, 20% test)\n",
    "n = len(demand_matrix)\n",
    "train_end = int(n * (1 - TEST_SIZE - VAL_SIZE))\n",
    "val_end = int(n * (1 - TEST_SIZE))\n",
    "\n",
    "train_data_all = demand_matrix.iloc[:train_end]\n",
    "val_data_all = demand_matrix.iloc[train_end:val_end]\n",
    "test_data_all = demand_matrix.iloc[val_end:]\n",
    "\n",
    "print(f\"\\nTrain-Test Split:\")\n",
    "print(f\"  Training: {len(train_data_all)} hours\")\n",
    "print(f\"  Validation: {len(val_data_all)} hours\")\n",
    "print(f\"  Test: {len(test_data_all)} hours\")\n",
    "\n",
    "# Save checkpoint\n",
    "demand_matrix.to_pickle(os.path.join(CHECKPOINT_PATH, '01_demand_matrix.pkl'))\n",
    "train_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_train_data.pkl'))\n",
    "val_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_val_data.pkl'))\n",
    "test_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_test_data.pkl'))\n",
    "\n",
    "print(f\"\\n✓ Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 2: Analyze Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Analyze Cluster Characteristics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate statistics for each cluster\n",
    "cluster_stats = pd.DataFrame({\n",
    "    'cluster_id': demand_matrix.columns,\n",
    "    'avg_hourly_demand': demand_matrix.mean(),\n",
    "    'median_hourly_demand': demand_matrix.median(),\n",
    "    'max_hourly_demand': demand_matrix.max(),\n",
    "    'std_hourly_demand': demand_matrix.std(),\n",
    "    'total_demand': demand_matrix.sum(),\n",
    "    'sparsity_pct': (demand_matrix == 0).sum() / len(demand_matrix) * 100,\n",
    "    'non_zero_hours': (demand_matrix != 0).sum()\n",
    "}).sort_values('total_demand', ascending=False).reset_index(drop=True)\n",
    "\n",
    "cluster_stats['demand_category'] = pd.cut(\n",
    "    cluster_stats['avg_hourly_demand'],\n",
    "    bins=[0, 10, 50, 500],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(f\"\\nCluster Demand Categories:\")\n",
    "print(f\"  High-demand (>50 trips/hour): {(cluster_stats['demand_category'] == 'High').sum()} clusters\")\n",
    "print(f\"  Medium-demand (10-50 trips/hour): {(cluster_stats['demand_category'] == 'Medium').sum()} clusters\")\n",
    "print(f\"  Low-demand (<10 trips/hour): {(cluster_stats['demand_category'] == 'Low').sum()} clusters\")\n",
    "\n",
    "# Save statistics\n",
    "cluster_stats.to_csv(os.path.join(OUTPUT_PATH, 'cluster_characteristics.csv'), index=False)\n",
    "print(f\"\\n✓ Cluster characteristics saved\")\n",
    "\n",
    "print(f\"\\nTop 10 clusters:\")\n",
    "print(cluster_stats.head(10)[['cluster_id', 'avg_hourly_demand', 'sparsity_pct', 'demand_category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 3: Train XGBoost on All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: XGBoost Training (All 100 Clusters)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_xgboost_features(data_df, n_lags=24):\n",
    "    \"\"\"Create lag features for XGBoost\"\"\"\n",
    "    df = data_df.copy()\n",
    "    \n",
    "    # Create lags\n",
    "    for col in data_df.columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df[f'{col}_lag_{lag}'] = data_df[col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for col in data_df.columns:\n",
    "        df[f'{col}_rolling_mean_6'] = data_df[col].shift(1).rolling(window=6).mean()\n",
    "        df[f'{col}_rolling_std_6'] = data_df[col].shift(1).rolling(window=6).std()\n",
    "        df[f'{col}_rolling_mean_24'] = data_df[col].shift(1).rolling(window=24).mean()\n",
    "    \n",
    "    # Temporal features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "print(\"\\nCreating XGBoost features...\")\n",
    "train_features = create_xgboost_features(train_data_all, N_LAGS)\n",
    "val_features = create_xgboost_features(val_data_all, N_LAGS)\n",
    "test_features = create_xgboost_features(test_data_all, N_LAGS)\n",
    "\n",
    "feature_cols = [col for col in train_features.columns if col not in demand_matrix.columns]\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Train XGBoost for each cluster\n",
    "xgb_models = {}\n",
    "xgb_metrics = {}\n",
    "xgb_predictions = pd.DataFrame(index=test_features.index)\n",
    "\n",
    "print(f\"\\nTraining {len(demand_matrix.columns)} XGBoost models...\")\n",
    "for i, cluster in enumerate(demand_matrix.columns):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Progress: {i+1}/100\")\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        train_features[feature_cols], \n",
    "        train_features[cluster],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    xgb_models[cluster] = model\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(test_features[feature_cols])\n",
    "    xgb_predictions[cluster] = pred\n",
    "    \n",
    "    # Evaluate\n",
    "    actual = test_features[cluster].values\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - pred[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    xgb_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Summary\n",
    "xgb_mape_values = [m['MAPE'] for m in xgb_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(f\"\\nXGBoost Summary (All 100 Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Std MAPE: {np.std(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Min MAPE: {np.min(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Max MAPE: {np.max(xgb_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(OUTPUT_PATH, 'xgboost_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_models, f)\n",
    "with open(os.path.join(OUTPUT_PATH, 'xgboost_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_metrics, f)\n",
    "\n",
    "del train_features, val_features, test_features\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ XGBoost models trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 4: Train AutoRegressive AR(7) on All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: AutoRegressive AR(7) Training (All 100 Clusters)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ar_models = {}\n",
    "ar_metrics = {}\n",
    "ar_predictions = pd.DataFrame(index=test_data_all.index)\n",
    "\n",
    "print(f\"\\nTraining {len(demand_matrix.columns)} AR(7) models...\")\n",
    "for i, cluster in enumerate(demand_matrix.columns):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Progress: {i+1}/100\")\n",
    "    \n",
    "    try:\n",
    "        model = ARIMA(train_data_all[cluster].dropna(), order=(7, 0, 0))\n",
    "        result = model.fit()\n",
    "        ar_models[cluster] = result\n",
    "        \n",
    "        # Forecast\n",
    "        forecast = result.get_forecast(steps=len(test_data_all)).predicted_mean.values\n",
    "        ar_predictions[cluster] = forecast\n",
    "        \n",
    "        # Evaluate\n",
    "        actual = test_data_all[cluster].values\n",
    "        rmse = np.sqrt(mean_squared_error(actual, forecast))\n",
    "        mae = mean_absolute_error(actual, forecast)\n",
    "        mask = actual != 0\n",
    "        mape = np.mean(np.abs((actual[mask] - forecast[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "        \n",
    "        ar_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "    except:\n",
    "        # Fallback: use mean forecast\n",
    "        ar_predictions[cluster] = train_data_all[cluster].mean()\n",
    "        ar_metrics[cluster] = {'RMSE': np.nan, 'MAE': np.nan, 'MAPE': np.nan}\n",
    "\n",
    "# Summary\n",
    "ar_mape_values = [m['MAPE'] for m in ar_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(f\"\\nAR(7) Summary (All 100 Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(ar_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(ar_mape_values):.2f}%\")\n",
    "print(f\"  Std MAPE: {np.std(ar_mape_values):.2f}%\")\n",
    "print(f\"  Min MAPE: {np.min(ar_mape_values):.2f}%\")\n",
    "print(f\"  Max MAPE: {np.max(ar_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(OUTPUT_PATH, 'ar_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(ar_models, f)\n",
    "with open(os.path.join(OUTPUT_PATH, 'ar_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(ar_metrics, f)\n",
    "\n",
    "print(f\"\\n✓ AR(7) models trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 5: Train ConvLSTM on Sampled High-Demand Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: ConvLSTM Training (Sample of High-Demand Clusters)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top 20 high-demand clusters for ConvLSTM\n",
    "high_demand_clusters = cluster_stats.head(20)['cluster_id'].tolist()\n",
    "\n",
    "print(f\"\\nConvLSTM on {len(high_demand_clusters)} high-demand clusters\")\n",
    "print(f\"  Reason: ConvLSTM computationally expensive, best on rich data\")\n",
    "\n",
    "# Prepare data\n",
    "train_convlstm = train_data_all[high_demand_clusters]\n",
    "val_convlstm = val_data_all[high_demand_clusters]\n",
    "test_convlstm = test_data_all[high_demand_clusters]\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(train_convlstm),\n",
    "    index=train_convlstm.index,\n",
    "    columns=train_convlstm.columns\n",
    ")\n",
    "val_scaled = pd.DataFrame(\n",
    "    scaler.transform(val_convlstm),\n",
    "    index=val_convlstm.index,\n",
    "    columns=val_convlstm.columns\n",
    ")\n",
    "test_scaled = pd.DataFrame(\n",
    "    scaler.transform(test_convlstm),\n",
    "    index=test_convlstm.index,\n",
    "    columns=test_convlstm.columns\n",
    ")\n",
    "\n",
    "# Create grid (5x4 = 20 clusters)\n",
    "def create_grid_data(data_df, grid_height=5, grid_width=4):\n",
    "    n_timesteps = len(data_df)\n",
    "    grid_data = np.zeros((n_timesteps, grid_height, grid_width, 1))\n",
    "    for idx, col in enumerate(data_df.columns):\n",
    "        row = idx // grid_width\n",
    "        col_pos = idx % grid_width\n",
    "        if row < grid_height:\n",
    "            grid_data[:, row, col_pos, 0] = data_df[col].values\n",
    "    return grid_data\n",
    "\n",
    "grid_train = create_grid_data(train_scaled, 5, 4)\n",
    "grid_val = create_grid_data(val_scaled, 5, 4)\n",
    "grid_test = create_grid_data(test_scaled, 5, 4)\n",
    "\n",
    "print(f\"\\nGrid shapes: {grid_train.shape}\")\n",
    "\n",
    "# Build ConvLSTM model\n",
    "input_layer = Input(shape=(6, 5, 4, 1), name='input')\n",
    "x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True, activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "output = Conv2D(1, (1, 1), padding='same', activation='relu', name='output')(x)\n",
    "\n",
    "convlstm_model = Model(inputs=input_layer, outputs=output)\n",
    "convlstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(f\"\\nTraining ConvLSTM...\")\n",
    "history = convlstm_model.fit(\n",
    "    grid_train, grid_train,\n",
    "    validation_data=(grid_val, grid_val),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict\n",
    "convlstm_pred = convlstm_model.predict(grid_test, verbose=0)\n",
    "\n",
    "# Extract and evaluate\n",
    "convlstm_metrics = {}\n",
    "convlstm_predictions = pd.DataFrame(index=test_convlstm.index)\n",
    "\n",
    "for idx, cluster in enumerate(high_demand_clusters):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    \n",
    "    pred_grid = convlstm_pred[:, row, col, 0]\n",
    "    convlstm_predictions[cluster] = scaler.inverse_transform(\n",
    "        np.column_stack([pred_grid] * len(train_convlstm.columns))\n",
    "    )[:, 0]\n",
    "    \n",
    "    actual = test_convlstm[cluster].values\n",
    "    rmse = np.sqrt(mean_squared_error(actual, convlstm_predictions[cluster].values))\n",
    "    mae = mean_absolute_error(actual, convlstm_predictions[cluster].values)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - convlstm_predictions[cluster].values[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    convlstm_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Summary\n",
    "convlstm_mape_values = [m['MAPE'] for m in convlstm_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(f\"\\nConvLSTM Summary ({len(high_demand_clusters)} High-Demand Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(convlstm_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(convlstm_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "convlstm_model.save(os.path.join(OUTPUT_PATH, 'convlstm_model.keras'))\n",
    "with open(os.path.join(OUTPUT_PATH, 'convlstm_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(convlstm_metrics, f)\n",
    "\n",
    "print(f\"\\n✓ ConvLSTM model trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 6: Comparative Analysis & Model Performance Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS: Which Model Works Best for Each Cluster?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for cluster in demand_matrix.columns:\n",
    "    row = {'cluster_id': cluster}\n",
    "    \n",
    "    # Get demand category\n",
    "    cluster_info = cluster_stats[cluster_stats['cluster_id'] == cluster].iloc[0]\n",
    "    row['avg_demand'] = cluster_info['avg_hourly_demand']\n",
    "    row['demand_category'] = cluster_info['demand_category']\n",
    "    row['sparsity_pct'] = cluster_info['sparsity_pct']\n",
    "    \n",
    "    # XGBoost metrics\n",
    "    if cluster in xgb_metrics:\n",
    "        row['xgboost_mape'] = xgb_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['xgboost_mape'] = np.nan\n",
    "    \n",
    "    # AR(7) metrics\n",
    "    if cluster in ar_metrics:\n",
    "        row['ar7_mape'] = ar_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['ar7_mape'] = np.nan\n",
    "    \n",
    "    # ConvLSTM metrics (only for top 20 clusters)\n",
    "    if cluster in convlstm_metrics:\n",
    "        row['convlstm_mape'] = convlstm_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['convlstm_mape'] = np.nan\n",
    "    \n",
    "    # Determine best model\n",
    "    mapes = {\n",
    "        'XGBoost': row['xgboost_mape'],\n",
    "        'AR(7)': row['ar7_mape'],\n",
    "        'ConvLSTM': row['convlstm_mape']\n",
    "    }\n",
    "    # Remove NaN values\n",
    "    mapes = {k: v for k, v in mapes.items() if not np.isnan(v)}\n",
    "    \n",
    "    if mapes:\n",
    "        row['best_model'] = min(mapes, key=mapes.get)\n",
    "        row['best_mape'] = min(mapes.values())\n",
    "    else:\n",
    "        row['best_model'] = 'N/A'\n",
    "        row['best_mape'] = np.nan\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(os.path.join(OUTPUT_PATH, 'model_comparison_all_clusters.csv'), index=False)\n",
    "\n",
    "print(f\"\\nComparison Results:\")\n",
    "print(f\"\\nBest Model Distribution:\")\n",
    "print(comparison_df['best_model'].value_counts())\n",
    "\n",
    "print(f\"\\nPerformance by Model (MAPE):\")\n",
    "print(f\"  XGBoost - Mean: {comparison_df['xgboost_mape'].mean():.2f}%, Median: {comparison_df['xgboost_mape'].median():.2f}%\")\n",
    "print(f\"  AR(7)   - Mean: {comparison_df['ar7_mape'].mean():.2f}%, Median: {comparison_df['ar7_mape'].median():.2f}%\")\n",
    "print(f\"  ConvLSTM - Mean: {comparison_df['convlstm_mape'].mean():.2f}%, Median: {comparison_df['convlstm_mape'].median():.2f}%\")\n",
    "\n",
    "print(f\"\\nBest Model by Demand Category:\")\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    subset = comparison_df[comparison_df['demand_category'] == category]\n",
    "    if len(subset) > 0:\n",
    "        best_counts = subset['best_model'].value_counts()\n",
    "        print(f\"  {category}: {dict(best_counts)}\")\n",
    "\n",
    "print(f\"\\n✓ Comparison saved to model_comparison_all_clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 7: Summary Tables & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY & INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "summary_stats = {\n",
    "    'Model': ['XGBoost', 'AR(7)', 'ConvLSTM (High-Demand Only)'],\n",
    "    'Clusters_Trained': [100, 100, 20],\n",
    "    'Mean_MAPE': [\n",
    "        comparison_df['xgboost_mape'].mean(),\n",
    "        comparison_df['ar7_mape'].mean(),\n",
    "        comparison_df['convlstm_mape'].mean()\n",
    "    ],\n",
    "    'Median_MAPE': [\n",
    "        comparison_df['xgboost_mape'].median(),\n",
    "        comparison_df['ar7_mape'].median(),\n",
    "        comparison_df['convlstm_mape'].median()\n",
    "    ],\n",
    "    'Std_MAPE': [\n",
    "        comparison_df['xgboost_mape'].std(),\n",
    "        comparison_df['ar7_mape'].std(),\n",
    "        comparison_df['convlstm_mape'].std()\n",
    "    ],\n",
    "    'Best_for_Count': [\n",
    "        (comparison_df['best_model'] == 'XGBoost').sum(),\n",
    "        (comparison_df['best_model'] == 'AR(7)').sum(),\n",
    "        (comparison_df['best_model'] == 'ConvLSTM').sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_PATH, 'model_performance_summary.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Insights\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FOR THESIS CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_best = (comparison_df['best_model'] == 'XGBoost').sum()\n",
    "ar_best = (comparison_df['best_model'] == 'AR(7)').sum()\n",
    "lstm_best = (comparison_df['best_model'] == 'ConvLSTM').sum()\n",
    "\n",
    "print(f\"\\n1. OVERALL WINNER:\")\n",
    "if xgb_best > ar_best and xgb_best > lstm_best:\n",
    "    print(f\"   XGBoost is best for {xgb_best} clusters ({xgb_best/100*100:.1f}%)\")\n",
    "elif ar_best > xgb_best and ar_best > lstm_best:\n",
    "    print(f\"   AR(7) is best for {ar_best} clusters ({ar_best/100*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ConvLSTM is best for {lstm_best} clusters ({lstm_best/100*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. MODEL SPECIALIZATION:\")\n",
    "high_dem = comparison_df[comparison_df['demand_category'] == 'High']\n",
    "med_dem = comparison_df[comparison_df['demand_category'] == 'Medium']\n",
    "low_dem = comparison_df[comparison_df['demand_category'] == 'Low']\n",
    "\n",
    "if len(high_dem) > 0:\n",
    "    print(f\"   High-demand zones: {high_dem['best_model'].value_counts().to_dict()}\")\n",
    "if len(med_dem) > 0:\n",
    "    print(f\"   Medium-demand zones: {med_dem['best_model'].value_counts().to_dict()}\")\n",
    "if len(low_dem) > 0:\n",
    "    print(f\"   Low-demand zones: {low_dem['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n3. SPARSE VS DENSE DATA:\")\n",
    "sparse = comparison_df[comparison_df['sparsity_pct'] > 30]\n",
    "dense = comparison_df[comparison_df['sparsity_pct'] <= 30]\n",
    "if len(sparse) > 0:\n",
    "    print(f\"   Sparse zones (>30% zeros): {sparse['best_model'].value_counts().to_dict()}\")\n",
    "if len(dense) > 0:\n",
    "    print(f\"   Dense zones (<=30% zeros): {dense['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n4. PERFORMANCE RANGE:\")\n",
    "print(f\"   XGBoost MAPE: {comparison_df['xgboost_mape'].min():.2f}% - {comparison_df['xgboost_mape'].max():.2f}%\")\n",
    "print(f\"   AR(7) MAPE: {comparison_df['ar7_mape'].min():.2f}% - {comparison_df['ar7_mape'].max():.2f}%\")\n",
    "print(f\"   ConvLSTM MAPE: {comparison_df['convlstm_mape'].min():.2f}% - {comparison_df['convlstm_mape'].max():.2f}%\")\n",
    "\n",
    "print(f\"\\n✓ Summary saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 8: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['XGBoost', 'AR(7)', 'ConvLSTM']\n",
    "means = [\n",
    "    comparison_df['xgboost_mape'].mean(),\n",
    "    comparison_df['ar7_mape'].mean(),\n",
    "    comparison_df['convlstm_mape'].mean()\n",
    "]\n",
    "stds = [\n",
    "    comparison_df['xgboost_mape'].std(),\n",
    "    comparison_df['ar7_mape'].std(),\n",
    "    comparison_df['convlstm_mape'].std()\n",
    "]\n",
    "ax.bar(models, means, yerr=stds, capsize=10, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Mean MAPE (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison (Mean ± Std)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Best Model Distribution\n",
    "ax = axes[0, 1]\n",
    "best_counts = comparison_df['best_model'].value_counts()\n",
    "colors_dict = {'XGBoost': '#3498db', 'AR(7)': '#e74c3c', 'ConvLSTM': '#2ecc71'}\n",
    "colors = [colors_dict.get(idx, '#95a5a6') for idx in best_counts.index]\n",
    "ax.bar(best_counts.index, best_counts.values, color=colors)\n",
    "ax.set_ylabel('Number of Clusters', fontsize=12)\n",
    "ax.set_title('Best Model Distribution (Which Model Wins for Each Cluster)', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(best_counts.values):\n",
    "    ax.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Performance by Demand Category\n",
    "ax = axes[1, 0]\n",
    "categories = ['High', 'Medium', 'Low']\n",
    "xgb_means = []\n",
    "ar_means = []\n",
    "lstm_means = []\n",
    "for cat in categories:\n",
    "    subset = comparison_df[comparison_df['demand_category'] == cat]\n",
    "    if len(subset) > 0:\n",
    "        xgb_means.append(subset['xgboost_mape'].mean())\n",
    "        ar_means.append(subset['ar7_mape'].mean())\n",
    "        lstm_means.append(subset['convlstm_mape'].mean())\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "ax.bar(x - width, xgb_means, width, label='XGBoost', color='#3498db')\n",
    "ax.bar(x, ar_means, width, label='AR(7)', color='#e74c3c')\n",
    "ax.bar(x + width, lstm_means, width, label='ConvLSTM', color='#2ecc71')\n",
    "ax.set_ylabel('Mean MAPE (%)', fontsize=12)\n",
    "ax.set_title('Performance by Demand Category', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. MAPE Distribution Box Plot\n",
    "ax = axes[1, 1]\n",
    "data_to_plot = [\n",
    "    comparison_df['xgboost_mape'].dropna(),\n",
    "    comparison_df['ar7_mape'].dropna(),\n",
    "    comparison_df['convlstm_mape'].dropna()\n",
    "]\n",
    "bp = ax.boxplot(data_to_plot, labels=['XGBoost', 'AR(7)', 'ConvLSTM'], patch_artist=True)\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax.set_title('MAPE Distribution by Model', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'model_comparison_visualization.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Visualizations saved\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"ALL ANALYSIS COMPLETE\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"\\nOutput files saved to: {OUTPUT_PATH}\")\n",
    "print(f\"\\nKey files:\")\n",
    "print(f\"  - model_comparison_all_clusters.csv (cluster-by-cluster comparison)\")\n",
    "print(f\"  - model_performance_summary.csv (overall statistics)\")\n",
    "print(f\"  - cluster_characteristics.csv (demand analysis)\")\n",
    "print(f\"  - model_comparison_visualization.png (charts)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
