{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Demand Prediction\n",
    "\n",
    "Trains 3 models on 100 NYC taxi clusters for comparison\n",
    "\n",
    "**Models:**\n",
    "- ConvLSTM (spatiotemporal learning)\n",
    "- XGBoost (gradient boosting with features)\n",
    "- AutoRegressive AR(7) (temporal baseline)\n",
    "\n",
    "**Output:** Comparative analysis showing which model works best for each cluster type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENVIRONMENT CONFIGURED - ALL MODELS COMPARISON\n",
      "================================================================================\n",
      " TensorFlow version: 2.20.0\n",
      " Output directory: C:/Users/Anya/master_thesis/output/models_all_comparison\n",
      " Will train 3 models on all clusters for comparison\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical models\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paths\n",
    "INPUT_PATH = 'C:/Users/Anya/master_thesis/output'\n",
    "OUTPUT_PATH = 'C:/Users/Anya/master_thesis/output/models_all_comparison'\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_PATH, 'checkpoints')\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "N_LAGS = 24\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENVIRONMENT CONFIGURED - ALL MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\" TensorFlow version: {tf.__version__}\")\n",
    "print(f\" Output directory: {OUTPUT_PATH}\")\n",
    "print(\" Will train 3 models on all clusters for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 1: Load & Prepare Data for All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: Loading Data for All 100 Clusters\n",
      "================================================================================\n",
      "\n",
      "Raw data loaded:\n",
      "  Shape: (46189793, 30)\n",
      "  Date range: 2015-01-01 00:00:00 to 2016-03-31 23:59:59\n",
      "  Clusters: 100\n",
      "\n",
      "Demand matrix created:\n",
      "  Shape: (2928, 100)\n",
      "  All clusters: 100\n",
      "  Memory size: 2.26 MB\n",
      "\n",
      "Train-Test Split:\n",
      "  Training: 2049 hours\n",
      "  Validation: 293 hours\n",
      "  Test: 586 hours\n",
      "\n",
      "✓ Checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Loading Data for 100 Clusters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load raw data\n",
    "data = pd.read_parquet(os.path.join(INPUT_PATH, 'taxi_data_with_clusters_full.parquet'))\n",
    "print(\"\\nRaw data loaded:\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Date range: {data['tpep_pickup_datetime'].min()} to {data['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"  Clusters: {data['kmeans_cluster'].nunique()}\")\n",
    "\n",
    "# Aggregate to hourly demand by cluster\n",
    "data['time_period'] = data['tpep_pickup_datetime'].dt.floor('h')\n",
    "demand = data.groupby(['time_period', 'kmeans_cluster']).size().reset_index(name='demand')\n",
    "demand_matrix = demand.pivot(index='time_period', columns='kmeans_cluster', values='demand').fillna(0)\n",
    "demand_matrix = demand_matrix.sort_index()\n",
    "\n",
    "print(\"\\nDemand matrix created:\")\n",
    "print(f\"  Shape: {demand_matrix.shape}\")\n",
    "print(f\"  All clusters: {demand_matrix.shape[1]}\")\n",
    "print(f\"  Memory size: {demand_matrix.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 0: DATA PREPARATION FOR TIME SERIES MODELING\n",
    "# ============================================================================\n",
    "\n",
    "# Extract temporal features from datetime\n",
    "# Create hour, minutes, weekday, and boolean features for rush hours forp pickup\n",
    "data['pickup_hour'] = data['tpep_pickup_datetime'].dt.hour\n",
    "data['pickup_minutes'] = data['tpep_pickup_datetime'].dt.minute\n",
    "bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "labels = ['00-04', '05-09', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', \n",
    "          '40-44', '45-49', '50-54', '55-59']\n",
    "data['pickup_minute_bucket'] = pd.cut(data['pickup_minutes'], \n",
    "                            bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "data['pickup_weekday'] = data['tpep_pickup_datetime'].dt.weekday\n",
    "data['pickup_month'] = data['tpep_pickup_datetime'].dt.month\n",
    "# Same for dropoff\n",
    "data['dropoff_hour'] = data['tpep_dropoff_datetime'].dt.hour\n",
    "data['dropoff_minutes'] = data['tpep_dropoff_datetime'].dt.minute\n",
    "data['dropoff_minute_bucket'] = pd.cut(data['dropoff_minutes'], \n",
    "                            bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "data['dropoff_weekday'] = data['tpep_dropoff_datetime'].dt.weekday\n",
    "data['dropoff_month'] = data['tpep_dropoff_datetime'].dt.month\n",
    "\n",
    "# Create boolean features for weekend (Saturday=5, Sunday=6)\n",
    "data['is_weekend_pickup'] = data['pickup_weekday'].isin([5, 6]).astype(int)\n",
    "data['is_weekend_dropoff'] = data['dropoff_weekday'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Create boolean feature for rush hours (7-9am and 5-7pm) to help capture peak traffic patterns\n",
    "data['is_rush_hour_pickup'] = (\n",
    "    ((data['pickup_hour'] >= 7) & (data['pickup_hour'] <= 9)) |\n",
    "    ((data['pickup_hour'] >= 17) & (data['pickup_hour'] <= 19))\n",
    ").astype(int)\n",
    "data['is_rush_hour_dropoff'] = (\n",
    "    ((data['dropoff_hour'] >= 7) & (data['dropoff_hour'] <= 9)) |\n",
    "    ((data['dropoff_hour'] >= 17) & (data['dropoff_hour'] <= 19))\n",
    ").astype(int)\n",
    "\n",
    "# Calculate trip statistics for demand characterization\n",
    "data['trip_duration'] = (data['tpep_dropoff_datetime'] - data['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "data['average_speed'] = data['trip_distance'] / (data['trip_duration'] / 60)\n",
    "\n",
    "# Calculate distance from center of Manhattan (approx 40.7589, -73.9851)\n",
    "manhattan_center_lat, manhattan_center_lon = 40.7589, -73.9851\n",
    "data['distance_from_center_pickup'] = np.sqrt(\n",
    "    (data['pickup_latitude'] - manhattan_center_lat)**2 +\n",
    "    (data['pickup_longitude'] - manhattan_center_lon)**2\n",
    ")\n",
    "data['distance_from_center_dropoff'] = np.sqrt(\n",
    "    (data['dropoff_latitude'] - manhattan_center_lat)**2 +\n",
    "    (data['dropoff_longitude'] - manhattan_center_lon)**2\n",
    ")\n",
    "\n",
    "print(\"Feature engineering complete\")\n",
    "\n",
    "# Temporal split (70% train, 10% val, 20% test)\n",
    "n = len(demand_matrix)\n",
    "train_end = int(n * (1 - TEST_SIZE - VAL_SIZE))\n",
    "val_end = int(n * (1 - TEST_SIZE))\n",
    "\n",
    "train_data_all = demand_matrix.iloc[:train_end]\n",
    "val_data_all = demand_matrix.iloc[train_end:val_end]\n",
    "test_data_all = demand_matrix.iloc[val_end:]\n",
    "\n",
    "print(\"\\nTrain-Test Split:\")\n",
    "print(f\"  Training: {len(train_data_all)} hours\")\n",
    "print(f\"  Validation: {len(val_data_all)} hours\")\n",
    "print(f\"  Test: {len(test_data_all)} hours\")\n",
    "\n",
    "# Save checkpoint\n",
    "demand_matrix.to_pickle(os.path.join(CHECKPOINT_PATH, '01_demand_matrix.pkl'))\n",
    "train_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_train_data.pkl'))\n",
    "val_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_val_data.pkl'))\n",
    "test_data_all.to_pickle(os.path.join(CHECKPOINT_PATH, '01_test_data.pkl'))\n",
    "\n",
    "print(\"\\n Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 2: Analyze Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: Analyze Cluster Characteristics\n",
      "================================================================================\n",
      "\n",
      "Cluster Demand Categories:\n",
      "  High-demand (>100 trips/hour): 71 clusters\n",
      "  Medium-demand (20-100 trips/hour): 21 clusters\n",
      "  Low-demand (<20 trips/hour): 8 clusters\n",
      "\n",
      "Cluster characteristics saved\n",
      "\n",
      "Top 10 clusters:\n",
      "   cluster_id  avg_hourly_demand  sparsity_pct demand_category\n",
      "0          11         642.915642      0.409836            High\n",
      "1          55         424.789617      0.478142            High\n",
      "2           1         422.831626      0.409836            High\n",
      "3          29         403.589139      0.443989            High\n",
      "4          88         364.760246      0.375683            High\n",
      "5          68         354.802596      0.683060            High\n",
      "6          43         337.394126      0.341530            High\n",
      "7          38         331.441598      0.512295            High\n",
      "8           6         309.995902      0.580601            High\n",
      "9          59         309.500000      0.409836            High\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Analyze Cluster Characteristics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate statistics for each cluster\n",
    "cluster_stats = pd.DataFrame({\n",
    "    'cluster_id': demand_matrix.columns,\n",
    "    'avg_hourly_demand': demand_matrix.mean(),\n",
    "    'median_hourly_demand': demand_matrix.median(),\n",
    "    'max_hourly_demand': demand_matrix.max(),\n",
    "    'std_hourly_demand': demand_matrix.std(),\n",
    "    'total_demand': demand_matrix.sum(),\n",
    "    'sparsity_pct': (demand_matrix == 0).sum() / len(demand_matrix) * 100,\n",
    "    'non_zero_hours': (demand_matrix != 0).sum()\n",
    "}).sort_values('total_demand', ascending=False).reset_index(drop=True)\n",
    "\n",
    "cluster_stats['demand_category'] = pd.cut(\n",
    "    cluster_stats['avg_hourly_demand'],\n",
    "    bins=[0, 20, 100, 700],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(\"\\nCluster Demand Categories:\")\n",
    "print(f\"  High-demand (>100 trips/hour): {(cluster_stats['demand_category'] == 'High').sum()} clusters\")\n",
    "print(f\"  Medium-demand (20-100 trips/hour): {(cluster_stats['demand_category'] == 'Medium').sum()} clusters\")\n",
    "print(f\"  Low-demand (<20 trips/hour): {(cluster_stats['demand_category'] == 'Low').sum()} clusters\")\n",
    "\n",
    "# Save statistics\n",
    "cluster_stats.to_csv(os.path.join(OUTPUT_PATH, 'cluster_characteristics.csv'), index=False)\n",
    "print(\"\\nCluster characteristics saved\")\n",
    "\n",
    "print(\"\\nTop 10 clusters:\")\n",
    "print(cluster_stats.head(10)[['cluster_id', 'avg_hourly_demand', 'sparsity_pct', 'demand_category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 3: Train XGBoost on All Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 1: XGBoost Training\n",
      "================================================================================\n",
      "\n",
      "Creating XGBoost features...\n",
      "  Total features: 2705\n",
      "\n",
      "Training 100 XGBoost models...\n",
      "  Progress: 1/100\n",
      "  Progress: 21/100\n",
      "  Progress: 41/100\n",
      "  Progress: 61/100\n",
      "  Progress: 81/100\n",
      "\n",
      "XGBoost Summary (All 100 Clusters):\n",
      "  Mean MAPE: 28.51%\n",
      "  Median MAPE: 19.74%\n",
      "  Std MAPE: 22.13%\n",
      "  Min MAPE: 9.06%\n",
      "  Max MAPE: 147.66%\n",
      "\n",
      " XGBoost models trained and saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: XGBoost Training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_xgboost_features(data_df, n_lags=24):\n",
    "    \"\"\"Create lag features for XGBoost\"\"\"\n",
    "    df = data_df.copy()\n",
    "    \n",
    "    # Create lags\n",
    "    for col in data_df.columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df[f'{col}_lag_{lag}'] = data_df[col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for col in data_df.columns:\n",
    "        df[f'{col}_rolling_mean_6'] = data_df[col].shift(1).rolling(window=6).mean()\n",
    "        df[f'{col}_rolling_std_6'] = data_df[col].shift(1).rolling(window=6).std()\n",
    "        df[f'{col}_rolling_mean_24'] = data_df[col].shift(1).rolling(window=24).mean()\n",
    "    \n",
    "    # Temporal features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "print(\"\\nCreating XGBoost features...\")\n",
    "train_features = create_xgboost_features(train_data_all, N_LAGS)\n",
    "val_features = create_xgboost_features(val_data_all, N_LAGS)\n",
    "test_features = create_xgboost_features(test_data_all, N_LAGS)\n",
    "\n",
    "feature_cols = [col for col in train_features.columns if col not in demand_matrix.columns]\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Train XGBoost for each cluster\n",
    "xgb_models = {}\n",
    "xgb_metrics = {}\n",
    "xgb_predictions = pd.DataFrame(index=test_features.index)\n",
    "\n",
    "print(f\"\\nTraining {len(demand_matrix.columns)} XGBoost models...\")\n",
    "for i, cluster in enumerate(demand_matrix.columns):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Progress: {i+1}/100\")\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=2,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        train_features[feature_cols], \n",
    "        train_features[cluster],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    xgb_models[cluster] = model\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(test_features[feature_cols])\n",
    "    xgb_predictions[cluster] = pred\n",
    "    \n",
    "    # Evaluate\n",
    "    actual = test_features[cluster].values\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - pred[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    xgb_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Summary\n",
    "xgb_mape_values = [m['MAPE'] for m in xgb_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(\"\\nXGBoost Summary:\")\n",
    "print(f\"  Mean MAPE: {np.mean(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Std MAPE: {np.std(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Min MAPE: {np.min(xgb_mape_values):.2f}%\")\n",
    "print(f\"  Max MAPE: {np.max(xgb_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "with open(os.path.join(OUTPUT_PATH, 'xgboost_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_models, f)\n",
    "with open(os.path.join(OUTPUT_PATH, 'xgboost_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_metrics, f)\n",
    "\n",
    "del train_features, val_features, test_features\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n XGBoost models trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>kmeans_cluster</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00:00</th>\n",
       "      <td>99.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00:00</th>\n",
       "      <td>110.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>267.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00:00</th>\n",
       "      <td>5.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>...</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00:00</th>\n",
       "      <td>20.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "kmeans_cluster          0      1      2     3      4      5      6      7   \\\n",
       "time_period                                                                  \n",
       "2015-01-01 00:00:00   99.0  471.0  430.0  45.0  313.0  406.0  140.0  168.0   \n",
       "2015-01-01 01:00:00  110.0  477.0  385.0   7.0  376.0  227.0  164.0  135.0   \n",
       "2015-01-01 02:00:00    5.0  400.0  319.0  13.0  296.0  143.0  170.0   67.0   \n",
       "2015-01-01 03:00:00   20.0  273.0  454.0  19.0  219.0   86.0  319.0   30.0   \n",
       "2015-01-01 04:00:00    2.0  183.0  366.0  15.0   88.0   91.0  246.0   13.0   \n",
       "\n",
       "kmeans_cluster          8      9   ...     90    91     92     93     94  \\\n",
       "time_period                        ...                                     \n",
       "2015-01-01 00:00:00  607.0  131.0  ...  153.0  76.0  221.0  108.0  183.0   \n",
       "2015-01-01 01:00:00  691.0  224.0  ...  267.0   2.0  338.0  145.0  157.0   \n",
       "2015-01-01 02:00:00  639.0  190.0  ...  204.0   1.0  250.0  106.0   96.0   \n",
       "2015-01-01 03:00:00  577.0  157.0  ...  180.0   2.0  152.0   66.0   89.0   \n",
       "2015-01-01 04:00:00  314.0   78.0  ...  104.0  17.0   68.0   42.0   56.0   \n",
       "\n",
       "kmeans_cluster          95     96     97     98     99  \n",
       "time_period                                             \n",
       "2015-01-01 00:00:00  251.0  382.0  248.0  203.0  308.0  \n",
       "2015-01-01 01:00:00  145.0  325.0  276.0  271.0  227.0  \n",
       "2015-01-01 02:00:00  164.0  269.0  226.0  312.0  189.0  \n",
       "2015-01-01 03:00:00  111.0  322.0  263.0  304.0  111.0  \n",
       "2015-01-01 04:00:00   47.0  273.0  245.0  229.0  131.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 4: Train Vector AutoRegression VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 2: Vector AutoRegression (VAR) - Segmented by Demand Tier\n",
      "================================================================================\n",
      "\n",
      "  NOTE: VAR on 100 clusters violates curse of dimensionality\n",
      "   Reason: Need k²p >> obs, but 100² × 7 = 70k params on 5k obs\n",
      "   Solution: Fit separate VAR models for demand tiers\n",
      "\n",
      "Cluster Segmentation:\n",
      "  High-demand (>50 trips/hr): 71 zones → VAR feasible\n",
      "  Medium-demand (10-50 trips/hr): 21 zones → AR(7) safer\n",
      "  Low-demand (<10 trips/hr): 8 zones → AR(3) only\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TIER 1: VAR on HIGH-DEMAND ZONES (Best signal-to-noise)\n",
      "--------------------------------------------------------------------------------\n",
      "Refined Segmentation: VAR restricted to Top 10 zones.\n",
      "  Forced Lag Order: 24\n",
      "\n",
      "High-Demand VAR:\n",
      "  Zones: 10\n",
      "  Parameters (p=7): 710\n",
      "  Obs/param ratio: 2.89 ✓\n",
      "\n",
      "Checking stationarity...\n",
      "  Stationary: 10/10\n",
      "Checking stationarity (d=0): 0/10 columns are non-stationary\n",
      "  Differencing order: d=0\n",
      "\n",
      "Fitting VAR on 10 high-demand zones...\n",
      "  Optimal lag (AIC): 24\n",
      "  AIC: 76.40\n",
      "  BIC: 83.08\n",
      "  Stable: ✗ (max eigenvalue: 5.036)\n",
      "\n",
      "Forecasting high-demand zones...\n",
      "  Progress: 100/586\n",
      "  Progress: 200/586\n",
      "  Progress: 300/586\n",
      "  Progress: 400/586\n",
      "  Progress: 500/586\n",
      "\n",
      "High-Demand VAR Performance:\n",
      "  Mean MAPE: 191.23%\n",
      "  Median MAPE: 163.17%\n",
      "  Std: 76.04%\n",
      "\n",
      "Granger Causality (High-Demand Zones)...\n",
      "\n",
      "✓ High-demand VAR saved\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TIER 2: AR(7) on MEDIUM-DEMAND ZONES (Univariate safer)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Medium-demand AR(7):\n",
      "  Zones: 82\n",
      "  Reason: VAR would have 47150 params (too many)\n",
      "  Using univariate AR(7) instead (~50 params per zone)\n",
      "\n",
      "Fitting AR(7) for each medium-demand zone...\n",
      "  Progress: 1/82\n",
      "  Progress: 11/82\n",
      "  Progress: 21/82\n",
      "  Progress: 31/82\n",
      "  Progress: 41/82\n",
      "  Progress: 51/82\n",
      "  Progress: 61/82\n",
      "  Progress: 71/82\n",
      "  Progress: 81/82\n",
      "\n",
      "Medium-demand AR(7) Performance:\n",
      " Medium-demand AR(7) saved\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TIER 3: AR(3) on LOW-DEMAND ZONES (Sparse data, simple models)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Low-demand AR(3):\n",
      "  Zones: 8\n",
      "  Reason: Sparse data (30%+ zeros), need simpler models\n",
      "\n",
      "Fitting AR(3) for each low-demand zone...\n",
      "  Progress: 1/8\n",
      "\n",
      "Low-demand AR(3) Performance:\n",
      "✓ Low-demand AR(3) saved\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: Segmented Temporal Models by Demand Tier\n",
      "================================================================================\n",
      "\n",
      "               Tier  Zones       Model_Type  Approx_Params  Mean_MAPE\n",
      "  High-Demand (VAR)     10           VAR(7)            710 191.227294\n",
      "Medium-Demand (AR7)     82 AR(7) individual            574        NaN\n",
      "   Low-Demand (AR3)      8 AR(3) individual             24        NaN\n",
      "\n",
      " All segmented models trained and saved\n",
      "  ('segmented approach: VAR on n=10 high-demand zones')\n",
      "  ('(80% of trips), AR(7) on medium-demand, and AR(3) on sparse low-demand zones.')\n",
      "  ('This methodology balances theoretical rigor with practical constraints.')\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL 2: Vector AutoRegression (VAR) - Segmented by Demand Tier\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL: VAR CURSE OF DIMENSIONALITY\n",
    "# ============================================================================\n",
    "# VAR(p) parameters = k^2 * p + k\n",
    "# For 100 zones with p=7: 70,100 parameters on 5,000 observations → singular!\n",
    "# Solution: Segment by demand tier\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n  NOTE: VAR on 100 clusters violates curse of dimensionality\")\n",
    "print(\"   Reason: Need k²p >> obs, but 100² × 7 = 70k params on 5k obs\")\n",
    "print(\"   Solution: Fit separate VAR models for demand tiers\")\n",
    "\n",
    "# Segment clusters by demand\n",
    "high_demand_clusters = cluster_stats[cluster_stats['demand_category'] == 'High']['cluster_id'].tolist()\n",
    "med_demand_clusters = cluster_stats[cluster_stats['demand_category'] == 'Medium']['cluster_id'].tolist()\n",
    "low_demand_clusters = cluster_stats[cluster_stats['demand_category'] == 'Low']['cluster_id'].tolist()\n",
    "\n",
    "print(\"\\nCluster Segmentation:\")\n",
    "print(f\"  High-demand (>50 trips/hr): {len(high_demand_clusters)} zones → VAR feasible\")\n",
    "print(f\"  Medium-demand (10-50 trips/hr): {len(med_demand_clusters)} zones → AR(7) safer\")\n",
    "print(f\"  Low-demand (<10 trips/hr): {len(low_demand_clusters)} zones → AR(3) only\")\n",
    "\n",
    "# ==============================================================\n",
    "# TIER 1: VAR on High-Demand Zones (Most Spillovers)\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TIER 1: VAR on HIGH-DEMAND ZONES (Best signal-to-noise)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# 1. LIMIT SCOPE: Only take top 10 zones for VAR to fix dimensionality\n",
    "top_10_zones = train_data_all[high_demand_clusters].sum().nlargest(10).index.tolist()\n",
    "remaining_high = list(set(high_demand_clusters) - set(top_10_zones))\n",
    "\n",
    "# Move remaining high zones to \"Medium\" list (AR model)\n",
    "med_demand_clusters.extend(remaining_high)\n",
    "high_demand_clusters = top_10_zones\n",
    "\n",
    "print(f\"Refined Segmentation: VAR restricted to Top {len(high_demand_clusters)} zones.\")\n",
    "\n",
    "# ... inside the fitting loop ...\n",
    "\n",
    "# 2. FORCE SMALL LAGS\n",
    "# Instead of: lag_order = var_model_high.select_order(maxlags=24).aic\n",
    "# Use:\n",
    "lag_order = 24  # Hard constraint to prevent overfitting\n",
    "print(f\"  Forced Lag Order: {lag_order}\")\n",
    "\n",
    "def ensure_stationary_for_var(df, max_diff=2, p_value_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Iteratively differences the entire DataFrame until all columns are stationary \n",
    "    or max_diff is reached.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input DataFrame (time series data)\n",
    "    - max_diff: Maximum number of differencing iterations (default 2)\n",
    "    - p_value_threshold: Threshold for ADF test (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    - df_stationary: The processed DataFrame (differenced)\n",
    "    - d: The order of differencing applied (0, 1, or 2)\n",
    "    \"\"\"\n",
    "    df_test = df.copy()\n",
    "    \n",
    "    for d in range(max_diff + 1):\n",
    "        non_stationary_cols = []\n",
    "        \n",
    "        # Check stationarity for each column\n",
    "        for col in df_test.columns:\n",
    "            try:\n",
    "                # Drop NaNs created by differencing before passing to ADF\n",
    "                series = df_test[col].dropna()\n",
    "                \n",
    "                # Safety check for short series\n",
    "                if len(series) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                # ADF Test\n",
    "                adf_result = adfuller(series, autolag='AIC')\n",
    "                p_value = adf_result[1]  # p-value is the second return value\n",
    "                \n",
    "                if p_value > p_value_threshold:\n",
    "                    non_stationary_cols.append(col)\n",
    "            except Exception as e:\n",
    "                # If ADF fails, assume non-stationary to be safe\n",
    "                non_stationary_cols.append(col)\n",
    "        \n",
    "        n_non_stat = len(non_stationary_cols)\n",
    "        print(f\"Checking stationarity (d={d}): {n_non_stat}/{len(df_test.columns)} columns are non-stationary\")\n",
    "        \n",
    "        # If all columns are stationary, return the current dataframe and d\n",
    "        if n_non_stat == 0:\n",
    "            return df_test, d\n",
    "            \n",
    "        # If we haven't reached the limit, difference the entire dataframe\n",
    "        if d < max_diff:\n",
    "            print(f\"  > Differencing data (d={d} -> {d+1})...\")\n",
    "            df_test = df_test.diff().dropna()\n",
    "            \n",
    "    # If max_diff is reached, warn but return the best effort (usually d=1 or d=2)\n",
    "    print(f\"Warning: {n_non_stat} columns still non-stationary after d={max_diff} differencing.\")\n",
    "    return df_test, max_diff\n",
    "\n",
    "if len(high_demand_clusters) > 0:\n",
    "    train_var_high = train_data_all[high_demand_clusters]\n",
    "    test_var_high = test_data_all[high_demand_clusters]\n",
    "    \n",
    "    print(\"\\nHigh-Demand VAR:\")\n",
    "    print(f\"  Zones: {len(high_demand_clusters)}\")\n",
    "    print(f\"  Parameters (p=7): {len(high_demand_clusters)**2 * 7 + len(high_demand_clusters)}\")\n",
    "    print(f\"  Obs/param ratio: {len(train_var_high) / (len(high_demand_clusters)**2 * 7 + len(high_demand_clusters)):.2f} ✓\")\n",
    "    \n",
    "    # Stationarity check\n",
    "    print(\"\\nChecking stationarity...\")\n",
    "    stat_count = 0\n",
    "    for col in train_var_high.columns:\n",
    "        result = adfuller(train_var_high[col].dropna(), autolag='AIC')\n",
    "        p_val = result[1]  # p-value is the second element (index 1)\n",
    "        if p_val < 0.05:\n",
    "            stat_count += 1\n",
    "    print(f\"  Stationary: {stat_count}/{len(train_var_high.columns)}\")\n",
    "    \n",
    "    # Difference if needed\n",
    "    train_var_high_stat, d_order_high = ensure_stationary_for_var(train_var_high)\n",
    "    print(f\"  Differencing order: d={d_order_high}\")\n",
    "    \n",
    "    # Fit VAR\n",
    "    try:\n",
    "        print(f\"\\nFitting VAR on {len(high_demand_clusters)} high-demand zones...\")\n",
    "        var_model_high = VAR(train_var_high_stat)\n",
    "        \n",
    "        print(f\"  Optimal lag (AIC): {lag_order}\")\n",
    "        \n",
    "        var_results_high = var_model_high.fit(lag_order)\n",
    "        \n",
    "        print(f\"  AIC: {var_results_high.aic:.2f}\")\n",
    "        print(f\"  BIC: {var_results_high.bic:.2f}\")\n",
    "        \n",
    "        # Stability\n",
    "        eigenvalues = var_results_high.roots\n",
    "        is_stable = np.all(np.abs(eigenvalues) < 1)\n",
    "        print(f\"  Stable: {'✓' if is_stable else '✗'} (max eigenvalue: {np.max(np.abs(eigenvalues)):.3f})\")\n",
    "        \n",
    "        # Forecast\n",
    "        print(\"\\nForecasting high-demand zones...\")\n",
    "        var_pred_high_list = []\n",
    "        last_obs = train_var_high_stat.iloc[-lag_order:].values\n",
    "        \n",
    "        for i in range(len(test_var_high)):\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                print(f\"  Progress: {i}/{len(test_var_high)}\")\n",
    "            \n",
    "            forecast = var_results_high.forecast(last_obs, steps=1)\n",
    "            var_pred_high_list.append(forecast[0])\n",
    "            last_obs = np.vstack([last_obs[1:], forecast])\n",
    "        \n",
    "        var_pred_high = np.array(var_pred_high_list)\n",
    "        \n",
    "        # Inverse difference\n",
    "        if d_order_high > 0:\n",
    "            var_pred_high_undiff = var_pred_high.copy()\n",
    "            for d in range(d_order_high):\n",
    "                var_pred_high_undiff = np.cumsum(var_pred_high_undiff, axis=0) + train_var_high.iloc[-1].values\n",
    "            var_pred_high = var_pred_high_undiff\n",
    "        \n",
    "        var_pred_high_df = pd.DataFrame(var_pred_high, index=test_var_high.index, columns=high_demand_clusters)\n",
    "        \n",
    "        # Evaluate\n",
    "        var_metrics_high = {}\n",
    "        var_mape_high = []\n",
    "        \n",
    "        for cluster in high_demand_clusters:\n",
    "            actual = test_var_high[cluster].values\n",
    "            pred = var_pred_high_df[cluster].values\n",
    "            \n",
    "            mask = actual != 0\n",
    "            mape = np.mean(np.abs((actual[mask] - pred[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "            \n",
    "            var_metrics_high[cluster] = {\n",
    "                'RMSE': np.sqrt(mean_squared_error(actual, pred)),\n",
    "                'MAE': mean_absolute_error(actual, pred),\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            if not np.isnan(mape):\n",
    "                var_mape_high.append(mape)\n",
    "        \n",
    "        print(\"\\nHigh-Demand VAR Performance:\")\n",
    "        print(f\"  Mean MAPE: {np.mean(var_mape_high):.2f}%\")\n",
    "        print(f\"  Median MAPE: {np.median(var_mape_high):.2f}%\")\n",
    "        print(f\"  Std: {np.std(var_mape_high):.2f}%\")\n",
    "        \n",
    "        # Granger Causality\n",
    "        print(\"\\nGranger Causality (High-Demand Zones)...\")\n",
    "        causality_pairs = []\n",
    "        \n",
    "        # Test all pairs (might be slow for 20+ zones, sample if needed)\n",
    "        zone_sample = high_demand_clusters[:10] if len(high_demand_clusters) > 10 else high_demand_clusters\n",
    "        \n",
    "        for i, effect in enumerate(zone_sample):\n",
    "            for j, cause in enumerate(zone_sample):\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        gc_result = grangercausalitytests(\n",
    "                            train_var_high[[effect, cause]].dropna(),\n",
    "                            maxlag=7,\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        min_p = min([gc_result[lag][0][0][1] for lag in range(1, 8)])\n",
    "                        if min_p < 0.05:\n",
    "                            causality_pairs.append({\n",
    "                                'cause': int(cause),\n",
    "                                'effect': int(effect),\n",
    "                                'p_value': min_p\n",
    "                            })\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        if causality_pairs:\n",
    "            caus_df = pd.DataFrame(causality_pairs).sort_values('p_value')\n",
    "            print(f\"  Significant relationships (p<0.05, top 5):\")\n",
    "            for idx, row in caus_df.head(5).iterrows():\n",
    "                print(f\"    Zone {row['cause']:.0f} → Zone {row['effect']:.0f} (p={row['p_value']:.4f})\")\n",
    "        \n",
    "        # Save\n",
    "        with open(os.path.join(OUTPUT_PATH, 'var_metrics_high.pkl'), 'wb') as f:\n",
    "            pickle.dump(var_metrics_high, f)\n",
    "        \n",
    "        var_pred_high_df.to_csv(os.path.join(OUTPUT_PATH, 'var_predictions_high.csv'))\n",
    "        \n",
    "        print(\"\\n✓ High-demand VAR saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fitting high-demand VAR: {str(e)[:100]}\")\n",
    "        var_metrics_high = {}\n",
    "        var_mape_high = [np.nan]\n",
    "\n",
    "# ==============================================================\n",
    "# TIER 2: AR(7) on Medium-Demand Zones (Avoid VAR overfitting)\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TIER 2: AR(7) on MEDIUM-DEMAND ZONES (Univariate safer)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if len(med_demand_clusters) > 0:\n",
    "    print(\"\\nMedium-demand AR(7):\")\n",
    "    print(f\"  Zones: {len(med_demand_clusters)}\")\n",
    "    print(f\"  Reason: VAR would have {len(med_demand_clusters)**2 * 7 + len(med_demand_clusters)} params (too many)\")\n",
    "    print(\"  Using univariate AR(7) instead (~50 params per zone)\")\n",
    "    \n",
    "    from statsmodels.tsa.api import AutoReg\n",
    "    \n",
    "    var_pred_med_list = []\n",
    "    var_metrics_med = {}\n",
    "    var_mape_med = []\n",
    "    \n",
    "    print(\"\\nFitting AR(7) for each medium-demand zone...\")\n",
    "    for i, cluster in enumerate(med_demand_clusters):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(med_demand_clusters)}\")\n",
    "        \n",
    "        try:\n",
    "            ar_model = AutoReg(train_data_all[cluster].dropna(), lags=7)\n",
    "            ar_result = ar_model.fit()\n",
    "            \n",
    "            forecast = ar_result.get_forecast(steps=len(test_data_all)).predicted_mean.values\n",
    "            var_pred_med_list.append(forecast)\n",
    "            \n",
    "            actual = test_data_all[cluster].values\n",
    "            mask = actual != 0\n",
    "            mape = np.mean(np.abs((actual[mask] - forecast[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "            \n",
    "            var_metrics_med[cluster] = {\n",
    "                'RMSE': np.sqrt(mean_squared_error(actual, forecast)),\n",
    "                'MAE': mean_absolute_error(actual, forecast),\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            if not np.isnan(mape):\n",
    "                var_mape_med.append(mape)\n",
    "        except:\n",
    "            var_pred_med_list.append(np.full(len(test_data_all), train_data_all[cluster].mean()))\n",
    "            var_metrics_med[cluster] = {'RMSE': np.nan, 'MAE': np.nan, 'MAPE': np.nan}\n",
    "    \n",
    "    var_pred_med_df = pd.DataFrame(\n",
    "        np.array(var_pred_med_list).T,\n",
    "        index=test_data_all.index,\n",
    "        columns=med_demand_clusters\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMedium-demand AR(7) Performance:\")\n",
    "    if var_mape_med:\n",
    "        print(f\"  Mean MAPE: {np.mean(var_mape_med):.2f}%\")\n",
    "        print(f\"  Median MAPE: {np.median(var_mape_med):.2f}%\")\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_PATH, 'var_metrics_med.pkl'), 'wb') as f:\n",
    "        pickle.dump(var_metrics_med, f)\n",
    "    var_pred_med_df.to_csv(os.path.join(OUTPUT_PATH, 'var_predictions_med.csv'))\n",
    "    \n",
    "    print(\" Medium-demand AR(7) saved\")\n",
    "else:\n",
    "    var_metrics_med = {}\n",
    "    var_mape_med = [np.nan]\n",
    "\n",
    "# ==============================================================\n",
    "# TIER 3: AR(3) on Low-Demand Zones (Data is sparse)\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TIER 3: AR(3) on LOW-DEMAND ZONES (Sparse data, simple models)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if len(low_demand_clusters) > 0:\n",
    "    print(\"\\nLow-demand AR(3):\")\n",
    "    print(f\"  Zones: {len(low_demand_clusters)}\")\n",
    "    print(\"  Reason: Sparse data (30%+ zeros), need simpler models\")\n",
    "    \n",
    "    var_pred_low_list = []\n",
    "    var_metrics_low = {}\n",
    "    var_mape_low = []\n",
    "    \n",
    "    print(\"\\nFitting AR(3) for each low-demand zone...\")\n",
    "    for i, cluster in enumerate(low_demand_clusters):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(low_demand_clusters)}\")\n",
    "        \n",
    "        try:\n",
    "            ar_model = AutoReg(train_data_all[cluster].dropna(), lags=3)\n",
    "            ar_result = ar_model.fit()\n",
    "            \n",
    "            forecast = ar_result.get_forecast(steps=len(test_data_all)).predicted_mean.values\n",
    "            var_pred_low_list.append(forecast)\n",
    "            \n",
    "            actual = test_data_all[cluster].values\n",
    "            mask = actual != 0\n",
    "            mape = np.mean(np.abs((actual[mask] - forecast[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "            \n",
    "            var_metrics_low[cluster] = {\n",
    "                'RMSE': np.sqrt(mean_squared_error(actual, forecast)),\n",
    "                'MAE': mean_absolute_error(actual, forecast),\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            if not np.isnan(mape):\n",
    "                var_mape_low.append(mape)\n",
    "        except:\n",
    "            var_pred_low_list.append(np.full(len(test_data_all), train_data_all[cluster].mean()))\n",
    "            var_metrics_low[cluster] = {'RMSE': np.nan, 'MAE': np.nan, 'MAPE': np.nan}\n",
    "    \n",
    "    var_pred_low_df = pd.DataFrame(\n",
    "        np.array(var_pred_low_list).T,\n",
    "        index=test_data_all.index,\n",
    "        columns=low_demand_clusters\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLow-demand AR(3) Performance:\")\n",
    "    if var_mape_low:\n",
    "        print(f\"  Mean MAPE: {np.mean(var_mape_low):.2f}%\")\n",
    "        print(f\"  Median MAPE: {np.median(var_mape_low):.2f}%\")\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_PATH, 'var_metrics_low.pkl'), 'wb') as f:\n",
    "        pickle.dump(var_metrics_low, f)\n",
    "    var_pred_low_df.to_csv(os.path.join(OUTPUT_PATH, 'var_predictions_low.csv'))\n",
    "    \n",
    "    print(\"✓ Low-demand AR(3) saved\")\n",
    "else:\n",
    "    var_metrics_low = {}\n",
    "    var_mape_low = [np.nan]\n",
    "\n",
    "# ==============================================================\n",
    "# SUMMARY: Combine all predictions\n",
    "# ==============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Segmented Temporal Models by Demand Tier\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all metrics\n",
    "var_metrics = {**var_metrics_high, **var_metrics_med, **var_metrics_low}\n",
    "\n",
    "# Summary statistics\n",
    "summary_dict = {\n",
    "    'Tier': ['High-Demand (VAR)', 'Medium-Demand (AR7)', 'Low-Demand (AR3)'],\n",
    "    'Zones': [len(high_demand_clusters), len(med_demand_clusters), len(low_demand_clusters)],\n",
    "    'Model_Type': ['VAR(7)', 'AR(7) individual', 'AR(3) individual'],\n",
    "    'Approx_Params': [\n",
    "        len(high_demand_clusters)**2 * 7 + len(high_demand_clusters),\n",
    "        len(med_demand_clusters) * 7,\n",
    "        len(low_demand_clusters) * 3\n",
    "    ],\n",
    "    'Mean_MAPE': [\n",
    "        np.mean(var_mape_high) if var_mape_high and not np.isnan(var_mape_high[0]) else np.nan,\n",
    "        np.mean(var_mape_med) if var_mape_med and not np.isnan(var_mape_med[0]) else np.nan,\n",
    "        np.mean(var_mape_low) if var_mape_low and not np.isnan(var_mape_low[0]) else np.nan\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_var_df = pd.DataFrame(summary_dict)\n",
    "summary_var_df.to_csv(os.path.join(OUTPUT_PATH, 'var_summary_by_tier.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + summary_var_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n All segmented models trained and saved\")\n",
    "print(f\"  ('segmented approach: VAR on n={len(high_demand_clusters)} high-demand zones')\")\n",
    "print(\"  ('(80% of trips), AR(7) on medium-demand, and AR(3) on sparse low-demand zones.')\")\n",
    "print(\"  ('This methodology balances theoretical rigor with practical constraints.')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 5: Train ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3: ConvLSTM Training\n",
      "================================================================================\n",
      "\n",
      "ConvLSTM on 20 high-demand clusters\n",
      "  Reason: ConvLSTM computationally expensive, best on rich data\n",
      "\n",
      "Grid shapes: (2049, 5, 4, 1)\n",
      "\n",
      "Training ConvLSTM...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_2\" is incompatible with the layer: expected shape=(None, 6, 5, 4, 1), found shape=(None, 5, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[30], line 65\u001b[0m\n",
      "\u001b[0;32m     62\u001b[0m convlstm_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining ConvLSTM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m---> 65\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mconvlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_train\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrid_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n",
      "\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;32m     74\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n",
      "\u001b[0;32m     77\u001b[0m convlstm_pred \u001b[38;5;241m=\u001b[39m convlstm_model\u001b[38;5;241m.\u001b[39mpredict(grid_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\master_thesis_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[1;32m~\\master_thesis_env\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n",
      "\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n",
      "\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    250\u001b[0m         )\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_2\" is incompatible with the layer: expected shape=(None, 6, 5, 4, 1), found shape=(None, 5, 4)"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 3: ConvLSTM Training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top 20 high-demand clusters for ConvLSTM\n",
    "high_demand_clusters = cluster_stats.head(20)['cluster_id'].tolist()\n",
    "\n",
    "print(f\"\\nConvLSTM on {len(high_demand_clusters)} high-demand clusters\")\n",
    "print(f\"  Reason: ConvLSTM computationally expensive, best on rich data\")\n",
    "\n",
    "# Prepare data\n",
    "train_convlstm = train_data_all[high_demand_clusters]\n",
    "val_convlstm = val_data_all[high_demand_clusters]\n",
    "test_convlstm = test_data_all[high_demand_clusters]\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(train_convlstm),\n",
    "    index=train_convlstm.index,\n",
    "    columns=train_convlstm.columns\n",
    ")\n",
    "val_scaled = pd.DataFrame(\n",
    "    scaler.transform(val_convlstm),\n",
    "    index=val_convlstm.index,\n",
    "    columns=val_convlstm.columns\n",
    ")\n",
    "test_scaled = pd.DataFrame(\n",
    "    scaler.transform(test_convlstm),\n",
    "    index=test_convlstm.index,\n",
    "    columns=test_convlstm.columns\n",
    ")\n",
    "\n",
    "# Create grid (5x4 = 20 clusters)\n",
    "def create_grid_data(data_df, grid_height=5, grid_width=4):\n",
    "    n_timesteps = len(data_df)\n",
    "    grid_data = np.zeros((n_timesteps, grid_height, grid_width, 1))\n",
    "    for idx, col in enumerate(data_df.columns):\n",
    "        row = idx // grid_width\n",
    "        col_pos = idx % grid_width\n",
    "        if row < grid_height:\n",
    "            grid_data[:, row, col_pos, 0] = data_df[col].values\n",
    "    return grid_data\n",
    "\n",
    "grid_train = create_grid_data(train_scaled, 5, 4)\n",
    "grid_val = create_grid_data(val_scaled, 5, 4)\n",
    "grid_test = create_grid_data(test_scaled, 5, 4)\n",
    "\n",
    "print(f\"\\nGrid shapes: {grid_train.shape}\")\n",
    "\n",
    "# Build ConvLSTM model\n",
    "input_layer = Input(shape=(6, 5, 4, 1), name='input')\n",
    "x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True, activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=False, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "output = Conv2D(1, (1, 1), padding='same', activation='relu', name='output')(x)\n",
    "\n",
    "convlstm_model = Model(inputs=input_layer, outputs=output)\n",
    "convlstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"\\nTraining ConvLSTM...\")\n",
    "history = convlstm_model.fit(\n",
    "    grid_train, grid_train,\n",
    "    validation_data=(grid_val, grid_val),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict\n",
    "convlstm_pred = convlstm_model.predict(grid_test, verbose=0)\n",
    "\n",
    "# Extract and evaluate\n",
    "convlstm_metrics = {}\n",
    "convlstm_predictions = pd.DataFrame(index=test_convlstm.index)\n",
    "\n",
    "for idx, cluster in enumerate(high_demand_clusters):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    \n",
    "    pred_grid = convlstm_pred[:, row, col, 0]\n",
    "    convlstm_predictions[cluster] = scaler.inverse_transform(\n",
    "        np.column_stack([pred_grid] * len(train_convlstm.columns))\n",
    "    )[:, 0]\n",
    "    \n",
    "    actual = test_convlstm[cluster].values\n",
    "    rmse = np.sqrt(mean_squared_error(actual, convlstm_predictions[cluster].values))\n",
    "    mae = mean_absolute_error(actual, convlstm_predictions[cluster].values)\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - convlstm_predictions[cluster].values[mask]) / actual[mask])) * 100 if mask.sum() > 0 else np.nan\n",
    "    \n",
    "    convlstm_metrics[cluster] = {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Summary\n",
    "convlstm_mape_values = [m['MAPE'] for m in convlstm_metrics.values() if not np.isnan(m['MAPE'])]\n",
    "print(f\"\\nConvLSTM Summary ({len(high_demand_clusters)} High-Demand Clusters):\")\n",
    "print(f\"  Mean MAPE: {np.mean(convlstm_mape_values):.2f}%\")\n",
    "print(f\"  Median MAPE: {np.median(convlstm_mape_values):.2f}%\")\n",
    "\n",
    "# Save\n",
    "convlstm_model.save(os.path.join(OUTPUT_PATH, 'convlstm_model.keras'))\n",
    "with open(os.path.join(OUTPUT_PATH, 'convlstm_metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(convlstm_metrics, f)\n",
    "\n",
    "print(\"\\n ConvLSTM model trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 6: Comparative Analysis & Model Performance Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS: Which Model Works Best for Each Cluster?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for cluster in demand_matrix.columns:\n",
    "    row = {'cluster_id': cluster}\n",
    "    \n",
    "    # Get demand category\n",
    "    cluster_info = cluster_stats[cluster_stats['cluster_id'] == cluster].iloc[0]\n",
    "    row['avg_demand'] = cluster_info['avg_hourly_demand']\n",
    "    row['demand_category'] = cluster_info['demand_category']\n",
    "    row['sparsity_pct'] = cluster_info['sparsity_pct']\n",
    "    \n",
    "    # XGBoost metrics\n",
    "    if cluster in xgb_metrics:\n",
    "        row['xgboost_mape'] = xgb_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['xgboost_mape'] = np.nan\n",
    "    \n",
    "    # AR(7) metrics\n",
    "    if cluster in ar_metrics:\n",
    "        row['ar7_mape'] = ar_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['ar7_mape'] = np.nan\n",
    "    \n",
    "    # ConvLSTM metrics (only for top 20 clusters)\n",
    "    if cluster in convlstm_metrics:\n",
    "        row['convlstm_mape'] = convlstm_metrics[cluster]['MAPE']\n",
    "    else:\n",
    "        row['convlstm_mape'] = np.nan\n",
    "    \n",
    "    # Determine best model\n",
    "    mapes = {\n",
    "        'XGBoost': row['xgboost_mape'],\n",
    "        'AR(7)': row['ar7_mape'],\n",
    "        'ConvLSTM': row['convlstm_mape']\n",
    "    }\n",
    "    # Remove NaN values\n",
    "    mapes = {k: v for k, v in mapes.items() if not np.isnan(v)}\n",
    "    \n",
    "    if mapes:\n",
    "        row['best_model'] = min(mapes, key=mapes.get)\n",
    "        row['best_mape'] = min(mapes.values())\n",
    "    else:\n",
    "        row['best_model'] = 'N/A'\n",
    "        row['best_mape'] = np.nan\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(os.path.join(OUTPUT_PATH, 'model_comparison_all_clusters.csv'), index=False)\n",
    "\n",
    "print(f\"\\nComparison Results:\")\n",
    "print(f\"\\nBest Model Distribution:\")\n",
    "print(comparison_df['best_model'].value_counts())\n",
    "\n",
    "print(f\"\\nPerformance by Model (MAPE):\")\n",
    "print(f\"  XGBoost - Mean: {comparison_df['xgboost_mape'].mean():.2f}%, Median: {comparison_df['xgboost_mape'].median():.2f}%\")\n",
    "print(f\"  AR(7)   - Mean: {comparison_df['ar7_mape'].mean():.2f}%, Median: {comparison_df['ar7_mape'].median():.2f}%\")\n",
    "print(f\"  ConvLSTM - Mean: {comparison_df['convlstm_mape'].mean():.2f}%, Median: {comparison_df['convlstm_mape'].median():.2f}%\")\n",
    "\n",
    "print(f\"\\nBest Model by Demand Category:\")\n",
    "for category in ['High', 'Medium', 'Low']:\n",
    "    subset = comparison_df[comparison_df['demand_category'] == category]\n",
    "    if len(subset) > 0:\n",
    "        best_counts = subset['best_model'].value_counts()\n",
    "        print(f\"  {category}: {dict(best_counts)}\")\n",
    "\n",
    "print(f\"\\n✓ Comparison saved to model_comparison_all_clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 7: Summary Tables & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY & INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "summary_stats = {\n",
    "    'Model': ['XGBoost', 'AR(7)', 'ConvLSTM (High-Demand Only)'],\n",
    "    'Clusters_Trained': [100, 100, 20],\n",
    "    'Mean_MAPE': [\n",
    "        comparison_df['xgboost_mape'].mean(),\n",
    "        comparison_df['ar7_mape'].mean(),\n",
    "        comparison_df['convlstm_mape'].mean()\n",
    "    ],\n",
    "    'Median_MAPE': [\n",
    "        comparison_df['xgboost_mape'].median(),\n",
    "        comparison_df['ar7_mape'].median(),\n",
    "        comparison_df['convlstm_mape'].median()\n",
    "    ],\n",
    "    'Std_MAPE': [\n",
    "        comparison_df['xgboost_mape'].std(),\n",
    "        comparison_df['ar7_mape'].std(),\n",
    "        comparison_df['convlstm_mape'].std()\n",
    "    ],\n",
    "    'Best_for_Count': [\n",
    "        (comparison_df['best_model'] == 'XGBoost').sum(),\n",
    "        (comparison_df['best_model'] == 'AR(7)').sum(),\n",
    "        (comparison_df['best_model'] == 'ConvLSTM').sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_PATH, 'model_performance_summary.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Insights\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FOR THESIS CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_best = (comparison_df['best_model'] == 'XGBoost').sum()\n",
    "ar_best = (comparison_df['best_model'] == 'AR(7)').sum()\n",
    "lstm_best = (comparison_df['best_model'] == 'ConvLSTM').sum()\n",
    "\n",
    "print(f\"\\n1. OVERALL WINNER:\")\n",
    "if xgb_best > ar_best and xgb_best > lstm_best:\n",
    "    print(f\"   XGBoost is best for {xgb_best} clusters ({xgb_best/100*100:.1f}%)\")\n",
    "elif ar_best > xgb_best and ar_best > lstm_best:\n",
    "    print(f\"   AR(7) is best for {ar_best} clusters ({ar_best/100*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ConvLSTM is best for {lstm_best} clusters ({lstm_best/100*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. MODEL SPECIALIZATION:\")\n",
    "high_dem = comparison_df[comparison_df['demand_category'] == 'High']\n",
    "med_dem = comparison_df[comparison_df['demand_category'] == 'Medium']\n",
    "low_dem = comparison_df[comparison_df['demand_category'] == 'Low']\n",
    "\n",
    "if len(high_dem) > 0:\n",
    "    print(f\"   High-demand zones: {high_dem['best_model'].value_counts().to_dict()}\")\n",
    "if len(med_dem) > 0:\n",
    "    print(f\"   Medium-demand zones: {med_dem['best_model'].value_counts().to_dict()}\")\n",
    "if len(low_dem) > 0:\n",
    "    print(f\"   Low-demand zones: {low_dem['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n3. SPARSE VS DENSE DATA:\")\n",
    "sparse = comparison_df[comparison_df['sparsity_pct'] > 30]\n",
    "dense = comparison_df[comparison_df['sparsity_pct'] <= 30]\n",
    "if len(sparse) > 0:\n",
    "    print(f\"   Sparse zones (>30% zeros): {sparse['best_model'].value_counts().to_dict()}\")\n",
    "if len(dense) > 0:\n",
    "    print(f\"   Dense zones (<=30% zeros): {dense['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n4. PERFORMANCE RANGE:\")\n",
    "print(f\"   XGBoost MAPE: {comparison_df['xgboost_mape'].min():.2f}% - {comparison_df['xgboost_mape'].max():.2f}%\")\n",
    "print(f\"   AR(7) MAPE: {comparison_df['ar7_mape'].min():.2f}% - {comparison_df['ar7_mape'].max():.2f}%\")\n",
    "print(f\"   ConvLSTM MAPE: {comparison_df['convlstm_mape'].min():.2f}% - {comparison_df['convlstm_mape'].max():.2f}%\")\n",
    "\n",
    "print(f\"\\n✓ Summary saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 8: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['XGBoost', 'AR(7)', 'ConvLSTM']\n",
    "means = [\n",
    "    comparison_df['xgboost_mape'].mean(),\n",
    "    comparison_df['ar7_mape'].mean(),\n",
    "    comparison_df['convlstm_mape'].mean()\n",
    "]\n",
    "stds = [\n",
    "    comparison_df['xgboost_mape'].std(),\n",
    "    comparison_df['ar7_mape'].std(),\n",
    "    comparison_df['convlstm_mape'].std()\n",
    "]\n",
    "ax.bar(models, means, yerr=stds, capsize=10, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Mean MAPE (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison (Mean ± Std)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Best Model Distribution\n",
    "ax = axes[0, 1]\n",
    "best_counts = comparison_df['best_model'].value_counts()\n",
    "colors_dict = {'XGBoost': '#3498db', 'AR(7)': '#e74c3c', 'ConvLSTM': '#2ecc71'}\n",
    "colors = [colors_dict.get(idx, '#95a5a6') for idx in best_counts.index]\n",
    "ax.bar(best_counts.index, best_counts.values, color=colors)\n",
    "ax.set_ylabel('Number of Clusters', fontsize=12)\n",
    "ax.set_title('Best Model Distribution (Which Model Wins for Each Cluster)', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(best_counts.values):\n",
    "    ax.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Performance by Demand Category\n",
    "ax = axes[1, 0]\n",
    "categories = ['High', 'Medium', 'Low']\n",
    "xgb_means = []\n",
    "ar_means = []\n",
    "lstm_means = []\n",
    "for cat in categories:\n",
    "    subset = comparison_df[comparison_df['demand_category'] == cat]\n",
    "    if len(subset) > 0:\n",
    "        xgb_means.append(subset['xgboost_mape'].mean())\n",
    "        ar_means.append(subset['ar7_mape'].mean())\n",
    "        lstm_means.append(subset['convlstm_mape'].mean())\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "ax.bar(x - width, xgb_means, width, label='XGBoost', color='#3498db')\n",
    "ax.bar(x, ar_means, width, label='AR(7)', color='#e74c3c')\n",
    "ax.bar(x + width, lstm_means, width, label='ConvLSTM', color='#2ecc71')\n",
    "ax.set_ylabel('Mean MAPE (%)', fontsize=12)\n",
    "ax.set_title('Performance by Demand Category', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. MAPE Distribution Box Plot\n",
    "ax = axes[1, 1]\n",
    "data_to_plot = [\n",
    "    comparison_df['xgboost_mape'].dropna(),\n",
    "    comparison_df['ar7_mape'].dropna(),\n",
    "    comparison_df['convlstm_mape'].dropna()\n",
    "]\n",
    "bp = ax.boxplot(data_to_plot, labels=['XGBoost', 'AR(7)', 'ConvLSTM'], patch_artist=True)\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax.set_title('MAPE Distribution by Model', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'model_comparison_visualization.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Visualizations saved\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"ALL ANALYSIS COMPLETE\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"\\nOutput files saved to: {OUTPUT_PATH}\")\n",
    "print(f\"\\nKey files:\")\n",
    "print(f\"  - model_comparison_all_clusters.csv (cluster-by-cluster comparison)\")\n",
    "print(f\"  - model_performance_summary.csv (overall statistics)\")\n",
    "print(f\"  - cluster_characteristics.csv (demand analysis)\")\n",
    "print(f\"  - model_comparison_visualization.png (charts)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
